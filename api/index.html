<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>API - LEMBAS</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "API";
        var mkdocs_page_input_path = "api.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> LEMBAS
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../macrophage_example/">Build the Model</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="./">API</a>
    <ul class="current">
    </ul>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">LEMBAS</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">API</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/hmbaghdassarian/LEMBAS/edit/master/docs/api.md">Edit on hmbaghdassarian/LEMBAS</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <div class="doc doc-object doc-module">



<a id="LEMBAS"></a>
  <div class="doc doc-contents first">

  

  <div class="doc doc-children">










<div class="doc doc-object doc-module">



<h2 id="LEMBAS.io" class="doc doc-heading">
          <code>io</code>


</h2>

  <div class="doc doc-contents ">
  
      <p>Read and write python objects.</p>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="LEMBAS.io.read_pickled_object" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">read_pickled_object</span><span class="p">(</span><span class="n">file_name</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Read an object as a pickled file.</p>
<h5 id="LEMBAS.io.read_pickled_object--parameters">Parameters</h5>
<p>file_name : str
    'full/path/to/file.pickle'</p>
<h5 id="LEMBAS.io.read_pickled_object--returns">Returns</h5>
<p>pickled_object
    the pickled object</p>

          <details class="quote">
            <summary>Source code in <code>LEMBAS/io.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">read_pickled_object</span><span class="p">(</span><span class="n">file_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Read an object as a pickled file.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    file_name : str</span>
<span class="sd">        &#39;full/path/to/file.pickle&#39;</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pickled_object</span>
<span class="sd">        the pickled object</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_name</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">handle</span><span class="p">:</span>
        <span class="n">pickled_object</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pickled_object</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="LEMBAS.io.write_pickled_object" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">write_pickled_object</span><span class="p">(</span><span class="nb">object</span><span class="p">,</span> <span class="n">file_name</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Save an object as a pickled file.</p>
<h5 id="LEMBAS.io.write_pickled_object--parameters">Parameters</h5>
<p>object : Any
    object to save
file_name : str
    'full/path/to/file.pickle'</p>

          <details class="quote">
            <summary>Source code in <code>LEMBAS/io.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">write_pickled_object</span><span class="p">(</span><span class="nb">object</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">file_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Save an object as a pickled file.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    object : Any</span>
<span class="sd">        object to save</span>
<span class="sd">    file_name : str</span>
<span class="sd">        &#39;full/path/to/file.pickle&#39;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">file_name</span><span class="p">:</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">file_name</span><span class="p">)</span>
        <span class="n">extensions</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">suffixes</span><span class="p">)</span>
        <span class="n">file_name</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">extensions</span><span class="p">,</span> <span class="s1">&#39;.pickle&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">file_name</span> <span class="o">=</span> <span class="n">file_name</span> <span class="o">+</span> <span class="s1">&#39;.pickle&#39;</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_name</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">handle</span><span class="p">:</span>
        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="nb">object</span><span class="p">,</span> <span class="n">handle</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="LEMBAS.model" class="doc doc-heading">
          <code>model</code>


</h2>

  <div class="doc doc-contents ">

  

  <div class="doc doc-children">










<div class="doc doc-object doc-module">



<h3 id="LEMBAS.model.activation_functions" class="doc doc-heading">
          <code>activation_functions</code>


</h3>

  <div class="doc doc-contents ">
  
      <p>Defines various activations functions to be used in neural net.</p>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h4 id="LEMBAS.model.activation_functions.MML_activation" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">MML_activation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">leak</span><span class="p">)</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Returns the output of the Michaelis-Menten function</p>
<h6 id="LEMBAS.model.activation_functions.MML_activation--parameters">Parameters</h6>
<p>x : torch.Tensor
    a vector of input values
leak : Union[float, int], optional
    parameter to tune extent of leaking, analogous to leaky ReLU</p>
<h6 id="LEMBAS.model.activation_functions.MML_activation--returns">Returns</h6>
<p>fx : torch.Tensor
    a vector of output values</p>

          <details class="quote">
            <summary>Source code in <code>LEMBAS/model/activation_functions.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">MML_activation</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">leak</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the output of the Michaelis-Menten function</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : torch.Tensor</span>
<span class="sd">        a vector of input values</span>
<span class="sd">    leak : Union[float, int], optional</span>
<span class="sd">        parameter to tune extent of leaking, analogous to leaky ReLU</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    fx : torch.Tensor</span>
<span class="sd">        a vector of output values</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="nb">input</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span> <span class="n">negative_slope</span> <span class="o">=</span> <span class="n">leak</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">shifted_x</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">fx</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">lt</span><span class="p">(</span><span class="n">shifted_x</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
    <span class="n">gated_x</span> <span class="o">=</span> <span class="n">fx</span> <span class="o">+</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">mask</span> <span class="c1">#prevents division by 0 issue on next line</span>
    <span class="n">right_values</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">shifted_x</span><span class="p">,</span> <span class="n">gated_x</span><span class="p">)</span>
    <span class="n">fx</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">*</span> <span class="p">(</span><span class="n">fx</span> <span class="o">-</span> <span class="n">right_values</span><span class="p">)</span> <span class="o">+</span> <span class="n">right_values</span> <span class="c1">#-fx trick from relu</span>
    <span class="k">return</span> <span class="n">fx</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h4 id="LEMBAS.model.activation_functions.MML_delta_activation" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">MML_delta_activation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">leak</span><span class="p">)</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Returns the derivative of the Michaelis-Menten function</p>
<h6 id="LEMBAS.model.activation_functions.MML_delta_activation--parameters">Parameters</h6>
<p>x : torch.Tensor
    a vector of input values
leak : Union[float, int], optional
    parameter to tune extent of leaking, analogous to leaky ReLU</p>
<h6 id="LEMBAS.model.activation_functions.MML_delta_activation--returns">Returns</h6>
<p>y : torch.Tensor
    a vector of output derivative values</p>

          <details class="quote">
            <summary>Source code in <code>LEMBAS/model/activation_functions.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">MML_delta_activation</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">leak</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the derivative of the Michaelis-Menten function</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : torch.Tensor</span>
<span class="sd">        a vector of input values</span>
<span class="sd">    leak : Union[float, int], optional</span>
<span class="sd">        parameter to tune extent of leaking, analogous to leaky ReLU</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    y : torch.Tensor</span>
<span class="sd">        a vector of output derivative values</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask1</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">le</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="c1">#derivative = 1 if nothing else is stated</span>

    <span class="n">mask2</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">right_values</span> <span class="o">=</span> <span class="mf">0.25</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">1e-12</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="c1"># add psuedocount bc x = 0 will creat NaN</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">+</span> <span class="n">mask2</span> <span class="o">*</span> <span class="n">right_values</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">leak</span><span class="p">)</span> <span class="o">*</span> <span class="n">mask1</span>
    <span class="k">return</span> <span class="n">y</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h4 id="LEMBAS.model.activation_functions.MML_onestepdelta_activation_factor" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">MML_onestepdelta_activation_factor</span><span class="p">(</span><span class="n">Y_full</span><span class="p">,</span> <span class="n">leak</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Adjusts weights for linearization in the spectral radius. </p>
<p>Note that this will only work for monotonic functions</p>
<h6 id="LEMBAS.model.activation_functions.MML_onestepdelta_activation_factor--parameters">Parameters</h6>
<p>Y_full : torch.Tensor
    <em>description</em>
leak : Union[float, int], optional
    parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01</p>
<h6 id="LEMBAS.model.activation_functions.MML_onestepdelta_activation_factor--returns">Returns</h6>
<p>y : torch.Tensor
    <em>description</em></p>

          <details class="quote">
            <summary>Source code in <code>LEMBAS/model/activation_functions.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">MML_onestepdelta_activation_factor</span><span class="p">(</span><span class="n">Y_full</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">leak</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Adjusts weights for linearization in the spectral radius. </span>

<span class="sd">    Note that this will only work for monotonic functions</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    Y_full : torch.Tensor</span>
<span class="sd">        _description_</span>
<span class="sd">    leak : Union[float, int], optional</span>
<span class="sd">        parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    y : torch.Tensor</span>
<span class="sd">        _description_</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">Y_full</span><span class="p">)</span>
    <span class="n">piece1</span> <span class="o">=</span> <span class="n">Y_full</span><span class="o">.</span><span class="n">le</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">piece3</span> <span class="o">=</span> <span class="n">Y_full</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="n">safe_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">Y_full</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">0.9999</span><span class="p">)</span>
    <span class="n">right_values</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">safe_x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">+</span> <span class="n">piece3</span> <span class="o">*</span> <span class="n">right_values</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">leak</span><span class="p">)</span> <span class="o">*</span> <span class="n">piece1</span>  
    <span class="k">return</span> <span class="n">y</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h4 id="LEMBAS.model.activation_functions.leakyReLU_activation" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">leakyReLU_activation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">leak</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Returns the output of the leaky ReLU function</p>
<h6 id="LEMBAS.model.activation_functions.leakyReLU_activation--parameters">Parameters</h6>
<p>x : Iterable[Union[float, int]]
    a vector of input values
leak : Union[float, int], optional
    parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01</p>
<h6 id="LEMBAS.model.activation_functions.leakyReLU_activation--returns">Returns</h6>
<p>fx
    a vector of output values</p>

          <details class="quote">
            <summary>Source code in <code>LEMBAS/model/activation_functions.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">leakyReLU_activation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">leak</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the output of the leaky ReLU function</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : Iterable[Union[float, int]]</span>
<span class="sd">        a vector of input values</span>
<span class="sd">    leak : Union[float, int], optional</span>
<span class="sd">        parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    fx</span>
<span class="sd">        a vector of output values</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">fx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">fx</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">fx</span> <span class="o">*</span> <span class="n">leak</span><span class="p">,</span> <span class="n">fx</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fx</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h4 id="LEMBAS.model.activation_functions.leakyReLU_delta_activation" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">leakyReLU_delta_activation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">leak</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Returns the derivative of the leaky ReLU function</p>
<h6 id="LEMBAS.model.activation_functions.leakyReLU_delta_activation--parameters">Parameters</h6>
<p>x : Iterable[Union[float, int]]
    a vector of input values
leak : Union[float, int], optional
    parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01</p>
<h6 id="LEMBAS.model.activation_functions.leakyReLU_delta_activation--returns">Returns</h6>
<p>y : Iterable[Union[float, int]]
    a vector of output derivative values</p>

          <details class="quote">
            <summary>Source code in <code>LEMBAS/model/activation_functions.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">leakyReLU_delta_activation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">leak</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the derivative of the leaky ReLU function</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : Iterable[Union[float, int]]</span>
<span class="sd">        a vector of input values</span>
<span class="sd">    leak : Union[float, int], optional</span>
<span class="sd">        parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    y : Iterable[Union[float, int]]</span>
<span class="sd">        a vector of output derivative values</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1">#derivative = 1 if nothing else is stated</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">leak</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1">#let derivative be 0.01 at x=0</span>
    <span class="k">return</span> <span class="n">y</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h4 id="LEMBAS.model.activation_functions.sigmoid_activation" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">sigmoid_activation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">leak</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Returns the output of the sigmoid function</p>
<h6 id="LEMBAS.model.activation_functions.sigmoid_activation--parameters">Parameters</h6>
<p>x : Iterable[Union[float, int]]
    a vector of input values
leak : Union[float, int], optional
    parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01</p>
<h6 id="LEMBAS.model.activation_functions.sigmoid_activation--returns">Returns</h6>
<p>fx
    a vector of output values</p>

          <details class="quote">
            <summary>Source code in <code>LEMBAS/model/activation_functions.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">sigmoid_activation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">leak</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the output of the sigmoid function</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : Iterable[Union[float, int]]</span>
<span class="sd">        a vector of input values</span>
<span class="sd">    leak : Union[float, int], optional</span>
<span class="sd">        parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    fx</span>
<span class="sd">        a vector of output values</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1">#leak is not used for sigmoid_</span>
    <span class="n">fx</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">fx</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">fx</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h4 id="LEMBAS.model.activation_functions.sigmoid_delta_activation" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">sigmoid_delta_activation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">leak</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Returns the derivative of the sigmoid function</p>
<h6 id="LEMBAS.model.activation_functions.sigmoid_delta_activation--parameters">Parameters</h6>
<p>x : Iterable[Union[float, int]]
    a vector of input values
leak : Union[float, int], optional
    parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01</p>
<h6 id="LEMBAS.model.activation_functions.sigmoid_delta_activation--returns">Returns</h6>
<p>y : Iterable[Union[float, int]]
    a vector of output derivative values</p>

          <details class="quote">
            <summary>Source code in <code>LEMBAS/model/activation_functions.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">sigmoid_delta_activation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">leak</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the derivative of the sigmoid function</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : Iterable[Union[float, int]]</span>
<span class="sd">        a vector of input values</span>
<span class="sd">    leak : Union[float, int], optional</span>
<span class="sd">        parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    y : Iterable[Union[float, int]]</span>
<span class="sd">        a vector of output derivative values</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">sigmoid_activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sigmoid_activation</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">y</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="LEMBAS.model.model_utilities" class="doc doc-heading">
          <code>model_utilities</code>


</h3>

  <div class="doc doc-contents ">
  
      <p>Helper functions for building the model.</p>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h4 id="LEMBAS.model.model_utilities.format_network" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">format_network</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">weight_label</span><span class="o">=</span><span class="s1">&#39;mode_of_action&#39;</span><span class="p">,</span> <span class="n">stimulation_label</span><span class="o">=</span><span class="s1">&#39;stimulation&#39;</span><span class="p">,</span> <span class="n">inhibition_label</span><span class="o">=</span><span class="s1">&#39;inhibition&#39;</span><span class="p">)</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Formats the standard network file format to that needed by <code>SignalingModel.parse_network</code></p>
<h6 id="LEMBAS.model.model_utilities.format_network--parameters">Parameters</h6>
<p>net : pd.DataFrame
    signaling network adjacency list with the following columns:
        - <code>weight_label</code>: whether the interaction is stimulating (1) or inhibiting (-1) or unknown (0.1). Exclude non-interacting (0) nodes. 
        - <code>stimulation_label</code>: binary whether an interaction is stimulating (1) or [not stimultaing or unknown] (0)
        - <code>inhibition_label</code>: binary whether an interaction is inhibiting (1) or [not inhibiting or unknown] (0)
weight_label : str, optional
    converts <code>stimulation_label</code> and <code>inhibition_label</code> to a single column of stimulating (1), inhibiting (-1), or
    unknown (0.1), by default 'mode_of_action'
stimulation_label : str, optional
    column name of stimulating interactions, see <code>net</code>, by default 'stimulation'
inhibition_label : str, optional
    column name of inhibitory interactions, see <code>net</code>, by default 'inhibition'</p>
<h6 id="LEMBAS.model.model_utilities.format_network--returns">Returns</h6>
<p>formatted_net : pd.DataFrame
    the same dataframe with the additional <code>weight_label</code> column</p>

          <details class="quote">
            <summary>Source code in <code>LEMBAS/model/model_utilities.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">format_network</span><span class="p">(</span><span class="n">net</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> 
                   <span class="n">weight_label</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mode_of_action&#39;</span><span class="p">,</span> 
                   <span class="n">stimulation_label</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;stimulation&#39;</span><span class="p">,</span> 
                   <span class="n">inhibition_label</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;inhibition&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Formats the standard network file format to that needed by `SignalingModel.parse_network`</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    net : pd.DataFrame</span>
<span class="sd">        signaling network adjacency list with the following columns:</span>
<span class="sd">            - `weight_label`: whether the interaction is stimulating (1) or inhibiting (-1) or unknown (0.1). Exclude non-interacting (0) nodes. </span>
<span class="sd">            - `stimulation_label`: binary whether an interaction is stimulating (1) or [not stimultaing or unknown] (0)</span>
<span class="sd">            - `inhibition_label`: binary whether an interaction is inhibiting (1) or [not inhibiting or unknown] (0)</span>
<span class="sd">    weight_label : str, optional</span>
<span class="sd">        converts `stimulation_label` and `inhibition_label` to a single column of stimulating (1), inhibiting (-1), or</span>
<span class="sd">        unknown (0.1), by default &#39;mode_of_action&#39;</span>
<span class="sd">    stimulation_label : str, optional</span>
<span class="sd">        column name of stimulating interactions, see `net`, by default &#39;stimulation&#39;</span>
<span class="sd">    inhibition_label : str, optional</span>
<span class="sd">        column name of inhibitory interactions, see `net`, by default &#39;inhibition&#39;</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    formatted_net : pd.DataFrame</span>
<span class="sd">        the same dataframe with the additional `weight_label` column</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">net</span><span class="p">[(</span><span class="n">net</span><span class="p">[</span><span class="n">stimulation_label</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="n">inhibition_label</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;An interaction can either be stimulating (1,0), inhibition (0,1) or unknown (0,0)&#39;</span><span class="p">)</span>

    <span class="n">formatted_net</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">formatted_net</span><span class="p">[</span><span class="n">weight_label</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">formatted_net</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">formatted_net</span><span class="p">[</span><span class="n">stimulation_label</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="n">weight_label</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">formatted_net</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">formatted_net</span><span class="p">[</span><span class="n">inhibition_label</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="n">weight_label</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

    <span class="c1">#ensuring that lack of known MOA does not imply lack of representation in scipy.sparse.find(A)</span>
    <span class="n">formatted_net</span><span class="p">[</span><span class="n">weight_label</span><span class="p">]</span> <span class="o">=</span> <span class="n">formatted_net</span><span class="p">[</span><span class="n">weight_label</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
    <span class="n">formatted_net</span><span class="p">[</span><span class="n">weight_label</span><span class="p">]</span> <span class="o">=</span> <span class="n">formatted_net</span><span class="p">[</span><span class="n">weight_label</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">formatted_net</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h4 id="LEMBAS.model.model_utilities.np_to_torch" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">np_to_torch</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Convert a numpy array to a torch.tensor</p>
<h6 id="LEMBAS.model.model_utilities.np_to_torch--parameters">Parameters</h6>
<p>arr : np.array</p>

<details class="dtype-" open>
  <summary>torch.dtype, optional</summary>
  <p>datatype to store values in torch, by default torch.float32</p>
</details>      <p>device : str
    whether to use gpu ("cuda") or cpu ("cpu"), by default "cpu"</p>

          <details class="quote">
            <summary>Source code in <code>LEMBAS/model/model_utilities.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">np_to_torch</span><span class="p">(</span><span class="n">arr</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;cpu&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert a numpy array to a torch.tensor</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    arr : np.array</span>

<span class="sd">    dtype : torch.dtype, optional</span>
<span class="sd">        datatype to store values in torch, by default torch.float32</span>
<span class="sd">    device : str</span>
<span class="sd">        whether to use gpu (&quot;cuda&quot;) or cpu (&quot;cpu&quot;), by default &quot;cpu&quot;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="LEMBAS.model.train" class="doc doc-heading">
          <code>train</code>


</h3>

  <div class="doc doc-contents ">
  
      <p>Train the signaling model.</p>

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h4 id="LEMBAS.model.train.ModelData" class="doc doc-heading">
          <code>ModelData</code>


</h4>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.utils.data.Dataset">Dataset</span></code></p>


            <details class="quote">
              <summary>Source code in <code>LEMBAS/model/train.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ModelData</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_in</span><span class="p">,</span> <span class="n">y_out</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X_in</span> <span class="o">=</span> <span class="n">X_in</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_out</span> <span class="o">=</span> <span class="n">y_out</span>
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="s2">&quot;Returns the total number of samples.&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="s2">&quot;Returns one sample of data, data and label (X, y).&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_in</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_out</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:]</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h5 id="LEMBAS.model.train.ModelData.__getitem__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__getitem__</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span></code>

</h5>


  <div class="doc doc-contents ">
  
      <p>Returns one sample of data, data and label (X, y).</p>

          <details class="quote">
            <summary>Source code in <code>LEMBAS/model/train.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="s2">&quot;Returns one sample of data, data and label (X, y).&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_in</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_out</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h5 id="LEMBAS.model.train.ModelData.__len__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__len__</span><span class="p">()</span></code>

</h5>


  <div class="doc doc-contents ">
  
      <p>Returns the total number of samples.</p>

          <details class="quote">
            <summary>Source code in <code>LEMBAS/model/train.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="s2">&quot;Returns the total number of samples.&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>



<div class="doc doc-object doc-function">



<h4 id="LEMBAS.model.train.split_data" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">split_data</span><span class="p">(</span><span class="n">X_in</span><span class="p">,</span> <span class="n">y_out</span><span class="p">,</span> <span class="n">train_split_frac</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;train&#39;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span> <span class="s1">&#39;validation&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">},</span> <span class="n">seed</span><span class="o">=</span><span class="mi">888</span><span class="p">)</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Splits the data into train, test, and validation.</p>
<h6 id="LEMBAS.model.train.split_data--parameters">Parameters</h6>
<p>X_in : torch.Tensor
    input ligand concentrations. Index represents samples and columns represent a ligand. Values represent amount of ligand introduced (e.g., concentration). 
y_out : torch.Tensor
    output TF activities. Index represents samples and columns represent TFs. Values represent activity of the TF.
train_split_frac : Dict, optional
    fraction of samples to be assigned to each of train, test and split, by default 0.8, 0.2, and 0 respectively
seed : int, optional
    seed value, by default 888</p>

          <details class="quote">
            <summary>Source code in <code>LEMBAS/model/train.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">split_data</span><span class="p">(</span><span class="n">X_in</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> 
               <span class="n">y_out</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> 
               <span class="n">train_split_frac</span><span class="p">:</span> <span class="n">Dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;train&#39;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span> <span class="s1">&#39;validation&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">},</span> 
              <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">888</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Splits the data into train, test, and validation.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X_in : torch.Tensor</span>
<span class="sd">        input ligand concentrations. Index represents samples and columns represent a ligand. Values represent amount of ligand introduced (e.g., concentration). </span>
<span class="sd">    y_out : torch.Tensor</span>
<span class="sd">        output TF activities. Index represents samples and columns represent TFs. Values represent activity of the TF.</span>
<span class="sd">    train_split_frac : Dict, optional</span>
<span class="sd">        fraction of samples to be assigned to each of train, test and split, by default 0.8, 0.2, and 0 respectively</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        seed value, by default 888</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="nb">sum</span><span class="p">([</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">train_split_frac</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="k">if</span> <span class="n">v</span><span class="p">]),</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Train-test-validation split must sum to 1&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">train_split_frac</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">]</span> <span class="ow">or</span> <span class="n">train_split_frac</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_in</span><span class="p">,</span> 
                                                        <span class="n">y_out</span><span class="p">,</span> 
                                                        <span class="n">train_size</span><span class="o">=</span><span class="n">train_split_frac</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span>
                                                        <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">X_train</span><span class="p">,</span> <span class="n">_X</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">_y</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_in</span><span class="p">,</span> 
                                                        <span class="n">y_out</span><span class="p">,</span> 
                                                        <span class="n">train_size</span><span class="o">=</span><span class="n">train_split_frac</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span>
                                                        <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">X_test</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">_X</span><span class="p">,</span> 
                                                    <span class="n">_y</span><span class="p">,</span> 
                                                    <span class="n">train_size</span><span class="o">=</span><span class="n">train_split_frac</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="n">train_split_frac</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">train_split_frac</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">]),</span>
                                                    <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">y_val</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h4 id="LEMBAS.model.train.train_signaling_model" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">train_signaling_model</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">reset_epoch</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">hyper_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">train_split_frac</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;train&#39;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span> <span class="s1">&#39;validation&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">},</span> <span class="n">train_seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Trains the signaling model</p>
<h6 id="LEMBAS.model.train.train_signaling_model--parameters">Parameters</h6>
<p>mod : SignalingModel
    initialized signaling model. Suggested to also run <code>mod.signaling_network.prescale_weights</code> prior to training
optimizer : torch.optim.adam.Adam
    optimizer to use during training
loss_fn : torch.nn.modules.loss.MSELoss
    loss function to use during training
reset_epoch : int, optional
    number of epochs upon which to reset the optimizer state, by default 200
hyper_params : Dict[str, Union[int, float]], optional
    various hyper parameter inputs for training
        - 'max_iter' : the number of epochs, by default 5000
        - 'learning_rate' : the starting learning rate, by default 2e-3
        - 'batch_size' : number of samples per batch, by default 8
        - 'noise_level' : noise added to signaling network input, by default 10. Set to 0 for no noise. Makes model more robust. 
        - 'gradient_noise_level' : noise added to gradient after backward pass. Makes model more robust. 
        - 'reset_epoch' : number of epochs upon which to reset the optimizer state, by default 200
        - 'param_lambda_L2' : L2 regularization penalty term for most of the model weights and biases
        - 'moa_lambda_L1' : L1 regularization penalty term for incorrect interaction mechanism of action (inhibiting/stimulating)
        - 'ligand_lambda_L2' : L2 regularization penalty term for ligand biases
        - 'uniform_lambda_L2' : L2 regularization penalty term for 
        - 'uniform_max' : 
        - 'spectral_loss_factor' : regularization penalty term for 
        - 'n_probes_spectral' : 
        - 'power_steps_spectral' : 
        - 'subset_n_spectral' : 
train_split_frac : Dict, optional
    fraction of samples to be assigned to each of train, test and split, by default 0.8, 0.2, and 0 respectively
train_seed : int, optional
    seed value, by default mod.seed. By explicitly making this an argument, it allows different train-test splits even 
    with the same mod.seed, e.g., for cross-validation
verbose : bool, optional
    whether to print various progress stats across training epochs</p>
<h6 id="LEMBAS.model.train.train_signaling_model--returns">Returns</h6>
<p>mod : SignalingModel
    a copy of the input model with trained parameters
cur_loss : List[float], optional
    a list of the loss (excluding regularizations) across training iterations
cur_eig : List[float], optional
    a list of the spectral_radius across training iterations
mean_loss : torch.Tensor
    mean TF activity loss across samples (independent of training)
X_train : torch.Tensor
    the train split of the input data
X_test : torch.Tensor
    the test split of the input data
X_val : torch.Tensor
    the validation split of the input data
y_train : torch.Tensor
    the train split of the output data
y_test : torch.Tensor
    the test split of the output data
y_val : torch.Tensor
    the validation split of the output data</p>

          <details class="quote">
            <summary>Source code in <code>LEMBAS/model/train.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">train_signaling_model</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span>  
                          <span class="n">optimizer</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="p">,</span> 
                          <span class="n">loss_fn</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span>
                          <span class="n">reset_epoch</span> <span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
                          <span class="n">hyper_params</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                          <span class="n">train_split_frac</span><span class="p">:</span> <span class="n">Dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;train&#39;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span> <span class="s1">&#39;validation&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">},</span>
                          <span class="n">train_seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                         <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Trains the signaling model</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    mod : SignalingModel</span>
<span class="sd">        initialized signaling model. Suggested to also run `mod.signaling_network.prescale_weights` prior to training</span>
<span class="sd">    optimizer : torch.optim.adam.Adam</span>
<span class="sd">        optimizer to use during training</span>
<span class="sd">    loss_fn : torch.nn.modules.loss.MSELoss</span>
<span class="sd">        loss function to use during training</span>
<span class="sd">    reset_epoch : int, optional</span>
<span class="sd">        number of epochs upon which to reset the optimizer state, by default 200</span>
<span class="sd">    hyper_params : Dict[str, Union[int, float]], optional</span>
<span class="sd">        various hyper parameter inputs for training</span>
<span class="sd">            - &#39;max_iter&#39; : the number of epochs, by default 5000</span>
<span class="sd">            - &#39;learning_rate&#39; : the starting learning rate, by default 2e-3</span>
<span class="sd">            - &#39;batch_size&#39; : number of samples per batch, by default 8</span>
<span class="sd">            - &#39;noise_level&#39; : noise added to signaling network input, by default 10. Set to 0 for no noise. Makes model more robust. </span>
<span class="sd">            - &#39;gradient_noise_level&#39; : noise added to gradient after backward pass. Makes model more robust. </span>
<span class="sd">            - &#39;reset_epoch&#39; : number of epochs upon which to reset the optimizer state, by default 200</span>
<span class="sd">            - &#39;param_lambda_L2&#39; : L2 regularization penalty term for most of the model weights and biases</span>
<span class="sd">            - &#39;moa_lambda_L1&#39; : L1 regularization penalty term for incorrect interaction mechanism of action (inhibiting/stimulating)</span>
<span class="sd">            - &#39;ligand_lambda_L2&#39; : L2 regularization penalty term for ligand biases</span>
<span class="sd">            - &#39;uniform_lambda_L2&#39; : L2 regularization penalty term for </span>
<span class="sd">            - &#39;uniform_max&#39; : </span>
<span class="sd">            - &#39;spectral_loss_factor&#39; : regularization penalty term for </span>
<span class="sd">            - &#39;n_probes_spectral&#39; : </span>
<span class="sd">            - &#39;power_steps_spectral&#39; : </span>
<span class="sd">            - &#39;subset_n_spectral&#39; : </span>
<span class="sd">    train_split_frac : Dict, optional</span>
<span class="sd">        fraction of samples to be assigned to each of train, test and split, by default 0.8, 0.2, and 0 respectively</span>
<span class="sd">    train_seed : int, optional</span>
<span class="sd">        seed value, by default mod.seed. By explicitly making this an argument, it allows different train-test splits even </span>
<span class="sd">        with the same mod.seed, e.g., for cross-validation</span>
<span class="sd">    verbose : bool, optional</span>
<span class="sd">        whether to print various progress stats across training epochs</span>


<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    mod : SignalingModel</span>
<span class="sd">        a copy of the input model with trained parameters</span>
<span class="sd">    cur_loss : List[float], optional</span>
<span class="sd">        a list of the loss (excluding regularizations) across training iterations</span>
<span class="sd">    cur_eig : List[float], optional</span>
<span class="sd">        a list of the spectral_radius across training iterations</span>
<span class="sd">    mean_loss : torch.Tensor</span>
<span class="sd">        mean TF activity loss across samples (independent of training)</span>
<span class="sd">    X_train : torch.Tensor</span>
<span class="sd">        the train split of the input data</span>
<span class="sd">    X_test : torch.Tensor</span>
<span class="sd">        the test split of the input data</span>
<span class="sd">    X_val : torch.Tensor</span>
<span class="sd">        the validation split of the input data</span>
<span class="sd">    y_train : torch.Tensor</span>
<span class="sd">        the train split of the output data</span>
<span class="sd">    y_test : torch.Tensor</span>
<span class="sd">        the test split of the output data</span>
<span class="sd">    y_val : torch.Tensor</span>
<span class="sd">        the validation split of the output data</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">hyper_params</span><span class="p">:</span>
        <span class="n">hyper_params</span> <span class="o">=</span> <span class="n">HYPER_PARAMS</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">hyper_params</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="p">{</span><span class="o">**</span><span class="n">HYPER_PARAMS</span><span class="p">,</span> <span class="o">**</span><span class="n">hyper_params</span><span class="p">}</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">HYPER_PARAMS</span><span class="p">}</span> <span class="c1"># give user input priority</span>

    <span class="n">stats</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">initialize_progress</span><span class="p">(</span><span class="n">hyper_params</span><span class="p">[</span><span class="s1">&#39;max_iter&#39;</span><span class="p">])</span>

    <span class="n">mod</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span> <span class="c1"># do not overwrite input</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">reset_state</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="n">X_in</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">df_to_tensor</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">X_in</span><span class="p">)</span>
    <span class="n">y_out</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">df_to_tensor</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">y_out</span><span class="p">)</span>
    <span class="n">mean_loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">y_out</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span> <span class="o">=</span> <span class="n">y_out</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">y_out</span><span class="p">)</span> <span class="c1"># mean TF (across samples) loss</span>

    <span class="c1"># set up data objects</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">train_seed</span><span class="p">:</span>
        <span class="n">train_seed</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">seed</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">split_data</span><span class="p">(</span><span class="n">X_in</span><span class="p">,</span> <span class="n">y_out</span><span class="p">,</span> <span class="n">train_split_frac</span><span class="p">,</span> <span class="n">train_seed</span><span class="p">)</span>

    <span class="n">train_data</span> <span class="o">=</span> <span class="n">ModelData</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">mod</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span><span class="p">:</span>
        <span class="n">pin_memory</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">pin_memory</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># if n_cores != 0:</span>
    <span class="c1">#     n_cores_train = min(n_cores, hyper_params[&#39;batch_size&#39;])</span>
    <span class="c1"># else:</span>
    <span class="c1">#     n_cores_train = n_cores</span>
    <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_data</span><span class="p">,</span>
                                  <span class="n">batch_size</span><span class="o">=</span><span class="n">hyper_params</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">],</span>
                                  <span class="c1"># num_workers=n_cores_train,</span>
                                  <span class="n">drop_last</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                                  <span class="n">pin_memory</span> <span class="o">=</span> <span class="n">pin_memory</span><span class="p">,</span>
                                  <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="c1"># begin iteration</span>
    <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">hyper_params</span><span class="p">[</span><span class="s1">&#39;max_iter&#39;</span><span class="p">]):</span>
        <span class="c1"># set learning rate</span>
        <span class="n">cur_lr</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_lr</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">hyper_params</span><span class="p">[</span><span class="s1">&#39;max_iter&#39;</span><span class="p">],</span> <span class="n">max_height</span> <span class="o">=</span> <span class="n">hyper_params</span><span class="p">[</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">],</span>
                              <span class="n">start_height</span><span class="o">=</span><span class="n">hyper_params</span><span class="p">[</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">]</span><span class="o">/</span><span class="mi">10</span><span class="p">,</span> <span class="n">end_height</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">peak</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">cur_lr</span>

        <span class="n">cur_loss</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">cur_eig</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># iterate through batches</span>
        <span class="k">if</span> <span class="n">mod</span><span class="o">.</span><span class="n">seed</span><span class="p">:</span>
            <span class="n">utils</span><span class="o">.</span><span class="n">set_seeds</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">seed</span> <span class="o">+</span> <span class="n">e</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">X_in_</span><span class="p">,</span> <span class="n">y_out_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
            <span class="n">mod</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

            <span class="n">X_in_</span><span class="p">,</span> <span class="n">y_out_</span> <span class="o">=</span> <span class="n">X_in_</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">y_out_</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># forward pass</span>
            <span class="n">X_full</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">input_layer</span><span class="p">(</span><span class="n">X_in_</span><span class="p">)</span> <span class="c1"># transform to full network with ligand input concentrations</span>
            <span class="n">utils</span><span class="o">.</span><span class="n">set_seeds</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">seed</span> <span class="o">+</span> <span class="n">mod</span><span class="o">.</span><span class="n">_gradient_seed_counter</span><span class="p">)</span>
            <span class="n">network_noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">X_full</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span> <span class="o">=</span> <span class="n">X_full</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">X_full</span> <span class="o">=</span> <span class="n">X_full</span> <span class="o">+</span> <span class="p">(</span><span class="n">hyper_params</span><span class="p">[</span><span class="s1">&#39;noise_level&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">cur_lr</span> <span class="o">*</span> <span class="n">network_noise</span><span class="p">)</span> <span class="c1"># randomly add noise to signaling network input, makes model more robust</span>
            <span class="n">Y_full</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">signaling_network</span><span class="p">(</span><span class="n">X_full</span><span class="p">)</span> <span class="c1"># train signaling network weights</span>
            <span class="n">Y_hat</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">output_layer</span><span class="p">(</span><span class="n">Y_full</span><span class="p">)</span>

            <span class="c1"># get prediction loss</span>
            <span class="n">fit_loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_out_</span><span class="p">,</span> <span class="n">Y_hat</span><span class="p">)</span>

            <span class="c1"># get regularization losses</span>
            <span class="n">sign_reg</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">signaling_network</span><span class="o">.</span><span class="n">sign_regularization</span><span class="p">(</span><span class="n">lambda_L1</span> <span class="o">=</span> <span class="n">hyper_params</span><span class="p">[</span><span class="s1">&#39;moa_lambda_L1&#39;</span><span class="p">])</span> <span class="c1"># incorrect MoA</span>
            <span class="n">ligand_reg</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">ligand_regularization</span><span class="p">(</span><span class="n">lambda_L2</span> <span class="o">=</span> <span class="n">hyper_params</span><span class="p">[</span><span class="s1">&#39;ligand_lambda_L2&#39;</span><span class="p">])</span> <span class="c1"># ligand biases</span>
            <span class="n">stability_loss</span><span class="p">,</span> <span class="n">spectral_radius</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">signaling_network</span><span class="o">.</span><span class="n">get_SS_loss</span><span class="p">(</span><span class="n">Y_full</span> <span class="o">=</span> <span class="n">Y_full</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">spectral_loss_factor</span> <span class="o">=</span> <span class="n">hyper_params</span><span class="p">[</span><span class="s1">&#39;spectral_loss_factor&#39;</span><span class="p">],</span>
                                                                                <span class="n">subset_n</span> <span class="o">=</span> <span class="n">hyper_params</span><span class="p">[</span><span class="s1">&#39;subset_n_spectral&#39;</span><span class="p">],</span> <span class="n">n_probes</span> <span class="o">=</span> <span class="n">hyper_params</span><span class="p">[</span><span class="s1">&#39;n_probes_spectral&#39;</span><span class="p">],</span> 
                                                                                <span class="n">power_steps</span> <span class="o">=</span> <span class="n">hyper_params</span><span class="p">[</span><span class="s1">&#39;power_steps_spectral&#39;</span><span class="p">])</span>
            <span class="n">uniform_reg</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">uniform_regularization</span><span class="p">(</span><span class="n">lambda_L2</span> <span class="o">=</span> <span class="n">hyper_params</span><span class="p">[</span><span class="s1">&#39;uniform_lambda_L2&#39;</span><span class="p">]</span><span class="o">*</span><span class="n">cur_lr</span><span class="p">,</span> <span class="n">Y_full</span> <span class="o">=</span> <span class="n">Y_full</span><span class="p">,</span> 
                                                     <span class="n">target_min</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">target_max</span> <span class="o">=</span> <span class="n">hyper_params</span><span class="p">[</span><span class="s1">&#39;uniform_max&#39;</span><span class="p">])</span> <span class="c1"># uniform distribution</span>
            <span class="n">param_reg</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">L2_reg</span><span class="p">(</span><span class="n">hyper_params</span><span class="p">[</span><span class="s1">&#39;param_lambda_L2&#39;</span><span class="p">])</span> <span class="c1"># all model weights and signaling network biases</span>

            <span class="n">total_loss</span> <span class="o">=</span> <span class="n">fit_loss</span> <span class="o">+</span> <span class="n">sign_reg</span> <span class="o">+</span> <span class="n">ligand_reg</span> <span class="o">+</span> <span class="n">param_reg</span> <span class="o">+</span> <span class="n">stability_loss</span> <span class="o">+</span> <span class="n">uniform_reg</span>

            <span class="c1"># gradient</span>
            <span class="n">total_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">mod</span><span class="o">.</span><span class="n">add_gradient_noise</span><span class="p">(</span><span class="n">noise_level</span> <span class="o">=</span> <span class="n">hyper_params</span><span class="p">[</span><span class="s1">&#39;gradient_noise_level&#39;</span><span class="p">])</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="c1"># store</span>
            <span class="n">cur_eig</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">spectral_radius</span><span class="p">)</span>
            <span class="n">cur_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fit_loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="n">stats</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">update_progress</span><span class="p">(</span><span class="n">stats</span><span class="p">,</span> <span class="nb">iter</span> <span class="o">=</span> <span class="n">e</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">cur_loss</span><span class="p">,</span> <span class="n">eig</span> <span class="o">=</span> <span class="n">cur_eig</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">cur_lr</span><span class="p">,</span> 
                                     <span class="n">n_sign_mismatches</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">signaling_network</span><span class="o">.</span><span class="n">count_sign_mismatch</span><span class="p">())</span>

        <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="n">e</span> <span class="o">%</span> <span class="mi">250</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">utils</span><span class="o">.</span><span class="n">print_stats</span><span class="p">(</span><span class="n">stats</span><span class="p">,</span> <span class="nb">iter</span> <span class="o">=</span> <span class="n">e</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">e</span> <span class="o">%</span> <span class="n">reset_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="n">e</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">):</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">reset_state</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="n">mins</span><span class="p">,</span> <span class="n">secs</span> <span class="o">=</span> <span class="nb">divmod</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">,</span> <span class="mi">60</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training ran in: </span><span class="si">{:.0f}</span><span class="s2"> min </span><span class="si">{:.2f}</span><span class="s2"> sec&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mins</span><span class="p">,</span> <span class="n">secs</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">mod</span><span class="p">,</span> <span class="n">cur_loss</span><span class="p">,</span> <span class="n">cur_eig</span><span class="p">,</span> <span class="n">mean_loss</span><span class="p">,</span> <span class="n">stats</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">y_val</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>


  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="LEMBAS.plotting" class="doc doc-heading">
          <code>plotting</code>


</h2>

  <div class="doc doc-contents ">
  
      <p>Helper functions for data visualization.</p>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="LEMBAS.plotting.shade_plot" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">shade_plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">x_label</span><span class="p">,</span> <span class="n">y_label</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p><em>summary</em></p>
<h5 id="LEMBAS.plotting.shade_plot--parameters">Parameters</h5>
<p>X : np.array
    x axis values
Y : np.array
    y axis values
sigma : np.array
    standard deviation of y axis values
x_label : str
    x axis label 
y_label : str
    y axis label</p>
<h5 id="LEMBAS.plotting.shade_plot--returns">Returns</h5>
<p>plot : plotnine.ggplot.ggplot
    <em>description</em></p>

          <details class="quote">
            <summary>Source code in <code>LEMBAS/plotting.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">shade_plot</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">sigma</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">x_label</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">y_label</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> 
              <span class="n">width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">height</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;_summary_</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : np.array</span>
<span class="sd">        x axis values</span>
<span class="sd">    Y : np.array</span>
<span class="sd">        y axis values</span>
<span class="sd">    sigma : np.array</span>
<span class="sd">        standard deviation of y axis values</span>
<span class="sd">    x_label : str</span>
<span class="sd">        x axis label </span>
<span class="sd">    y_label : str</span>
<span class="sd">        y axis label</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    plot : plotnine.ggplot.ggplot</span>
<span class="sd">        _description_</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">x_label</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span> <span class="n">y_label</span><span class="p">:</span> <span class="n">Y</span><span class="p">,</span> <span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="n">sigma</span>
    <span class="p">})</span>
    <span class="n">data</span><span class="p">[</span><span class="s1">&#39;sigma_min&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">y_label</span><span class="p">]</span> <span class="o">-</span> <span class="n">data</span><span class="o">.</span><span class="n">sigma</span>
    <span class="n">data</span><span class="p">[</span><span class="s1">&#39;sigma_max&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">y_label</span><span class="p">]</span> <span class="o">+</span> <span class="n">data</span><span class="o">.</span><span class="n">sigma</span>

    <span class="n">plot</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">p9</span><span class="o">.</span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">p9</span><span class="o">.</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_label</span><span class="p">))</span> <span class="o">+</span>
        <span class="n">p9</span><span class="o">.</span><span class="n">geom_line</span><span class="p">(</span><span class="n">p9</span><span class="o">.</span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">y_label</span><span class="p">),</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;#1E90FF&#39;</span><span class="p">)</span> <span class="o">+</span>
        <span class="n">p9</span><span class="o">.</span><span class="n">geom_ribbon</span><span class="p">(</span><span class="n">p9</span><span class="o">.</span><span class="n">aes</span><span class="p">(</span><span class="n">y</span> <span class="o">=</span> <span class="n">y_label</span><span class="p">,</span> <span class="n">ymin</span> <span class="o">=</span> <span class="s1">&#39;sigma_min&#39;</span><span class="p">,</span> <span class="n">ymax</span> <span class="o">=</span> <span class="s1">&#39;sigma_max&#39;</span><span class="p">),</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="o">+</span>
        <span class="n">p9</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span> <span class="o">+</span>
        <span class="c1"># p9.ylim(10**min_log, round(data[y_label].max(), 1)) +</span>
        <span class="c1"># p9.scale_y_log10() + </span>
        <span class="n">p9</span><span class="o">.</span><span class="n">theme_bw</span><span class="p">()</span> <span class="o">+</span> 
        <span class="n">p9</span><span class="o">.</span><span class="n">theme</span><span class="p">(</span><span class="n">figure_size</span><span class="o">=</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">))</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">plot</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="LEMBAS.utilities" class="doc doc-heading">
          <code>utilities</code>


</h2>

  <div class="doc doc-contents ">
  
      <p>Helper functions for running and training the SignalingModel.</p>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="LEMBAS.utilities.get_lr" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_lr</span><span class="p">(</span><span class="nb">iter</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span> <span class="n">max_height</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">start_height</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">end_height</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">peak</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Calculates learning rate for a given iteration during training.</p>
<h5 id="LEMBAS.utilities.get_lr--parameters">Parameters</h5>
<p>iter : int
    the current iteration
max_iter : int
    the maximum number of training iterations
max_height : float, optional
    tuning parameters for learning for the first 95% of iterations, by default 1e-3
start_height : float, optional
    tuning parameter for learning rate before peak iterations, by default 1e-5
end_height : float, optional
    tuning parameter for learning rate afer peak iterations, by default 1e-5
peak : int, optional
    the first # of iterations to calculate lr on (should be less than 95% 
    of max_iter), by default 1000</p>
<h5 id="LEMBAS.utilities.get_lr--returns">Returns</h5>
<p>lr : float
    the learning rate</p>

          <details class="quote">
            <summary>Source code in <code>LEMBAS/utilities.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_lr</span><span class="p">(</span><span class="nb">iter</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_height</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span> 
             <span class="n">start_height</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">end_height</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> 
             <span class="n">peak</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculates learning rate for a given iteration during training.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    iter : int</span>
<span class="sd">        the current iteration</span>
<span class="sd">    max_iter : int</span>
<span class="sd">        the maximum number of training iterations</span>
<span class="sd">    max_height : float, optional</span>
<span class="sd">        tuning parameters for learning for the first 95% of iterations, by default 1e-3</span>
<span class="sd">    start_height : float, optional</span>
<span class="sd">        tuning parameter for learning rate before peak iterations, by default 1e-5</span>
<span class="sd">    end_height : float, optional</span>
<span class="sd">        tuning parameter for learning rate afer peak iterations, by default 1e-5</span>
<span class="sd">    peak : int, optional</span>
<span class="sd">        the first # of iterations to calculate lr on (should be less than 95% </span>
<span class="sd">        of max_iter), by default 1000</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    lr : float</span>
<span class="sd">        the learning rate</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">phase_length</span> <span class="o">=</span> <span class="mf">0.95</span> <span class="o">*</span> <span class="n">max_iter</span>
    <span class="k">if</span> <span class="nb">iter</span><span class="o">&lt;=</span><span class="n">peak</span><span class="p">:</span>
        <span class="n">effective_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="o">/</span><span class="n">peak</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_height</span><span class="o">-</span><span class="n">start_height</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="p">(</span><span class="n">effective_iter</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">start_height</span>
    <span class="k">elif</span> <span class="nb">iter</span><span class="o">&lt;=</span><span class="n">phase_length</span><span class="p">:</span>
        <span class="n">effective_iter</span> <span class="o">=</span> <span class="p">(</span><span class="nb">iter</span><span class="o">-</span><span class="n">peak</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">phase_length</span><span class="o">-</span><span class="n">peak</span><span class="p">)</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_height</span><span class="o">-</span><span class="n">end_height</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="p">(</span><span class="n">effective_iter</span><span class="o">+</span><span class="mi">2</span><span class="p">))</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">end_height</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">end_height</span>
    <span class="k">return</span> <span class="n">lr</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="LEMBAS.utilities.get_moving_average" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_moving_average</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Get the moving average of a tracked state across n_steps. Serves to smooth value. </p>
<h5 id="LEMBAS.utilities.get_moving_average--parameters">Parameters</h5>
<p>values : np.array
    values on which to get the moving average
n_steps : int
    number of steps across which to get the moving average</p>
<h5 id="LEMBAS.utilities.get_moving_average--returns">Returns</h5>
<p>moving_average : np.array
    the moving average across values</p>

          <details class="quote">
            <summary>Source code in <code>LEMBAS/utilities.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_moving_average</span><span class="p">(</span><span class="n">values</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the moving average of a tracked state across n_steps. Serves to smooth value. </span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    values : np.array</span>
<span class="sd">        values on which to get the moving average</span>
<span class="sd">    n_steps : int</span>
<span class="sd">        number of steps across which to get the moving average</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    moving_average : np.array</span>
<span class="sd">        the moving average across values</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">moving_average</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">values</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">values</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">((</span><span class="n">i</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">n_steps</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">stop</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">((</span><span class="n">i</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">n_steps</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="n">values</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">moving_average</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">stop</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">moving_average</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="LEMBAS.utilities.initialize_progress" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">initialize_progress</span><span class="p">(</span><span class="n">max_iter</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Track various stats of the progress of training the model.</p>
<h5 id="LEMBAS.utilities.initialize_progress--parameters">Parameters</h5>
<p>max_iter : int
    the maximum number of training iterations</p>
<h5 id="LEMBAS.utilities.initialize_progress--returns">Returns</h5>
<p>stats : dict
    a dictionary of progress statistics</p>

          <details class="quote">
            <summary>Source code in <code>LEMBAS/utilities.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">initialize_progress</span><span class="p">(</span><span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Track various stats of the progress of training the model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    max_iter : int</span>
<span class="sd">        the maximum number of training iterations</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    stats : dict</span>
<span class="sd">        a dictionary of progress statistics</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">stats</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;start_time&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;end_time&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;iter_time&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">max_iter</span><span class="p">)</span>

    <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;loss_mean&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">max_iter</span><span class="p">)</span>
    <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;loss_sigma&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">max_iter</span><span class="p">)</span>
    <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;eig_mean&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">max_iter</span><span class="p">)</span>
    <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;eig_sigma&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">max_iter</span><span class="p">)</span>

    <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">max_iter</span><span class="p">)</span>
    <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">max_iter</span><span class="p">)</span>
    <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;violations&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">max_iter</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">stats</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="LEMBAS.utilities.print_stats" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">print_stats</span><span class="p">(</span><span class="n">stats</span><span class="p">,</span> <span class="nb">iter</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Prints various stats of the progress of training the model.</p>
<h5 id="LEMBAS.utilities.print_stats--parameters">Parameters</h5>
<p>stats : dict
    a dictionary of progress statistics
iter : int
    the current training iteration</p>

          <details class="quote">
            <summary>Source code in <code>LEMBAS/utilities.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">print_stats</span><span class="p">(</span><span class="n">stats</span><span class="p">,</span> <span class="nb">iter</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Prints various stats of the progress of training the model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    stats : dict</span>
<span class="sd">        a dictionary of progress statistics</span>
<span class="sd">    iter : int</span>
<span class="sd">        the current training iteration</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">msg</span> <span class="o">=</span> <span class="s1">&#39;i=</span><span class="si">{:.0f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">iter</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;loss_mean&#39;</span><span class="p">][</span><span class="nb">iter</span><span class="p">]):</span>
        <span class="n">msg</span> <span class="o">+=</span> <span class="s1">&#39;, l=</span><span class="si">{:.5f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;loss_mean&#39;</span><span class="p">][</span><span class="nb">iter</span><span class="p">])</span>
    <span class="c1"># if not np.isnan(stats[&#39;test&#39;][iter]):</span>
    <span class="c1">#     msg += &#39;, t={:.5f}&#39;.format(stats[&#39;test&#39;][iter])</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;eig_mean&#39;</span><span class="p">][</span><span class="nb">iter</span><span class="p">]):</span>
        <span class="n">msg</span> <span class="o">+=</span> <span class="s1">&#39;, s=</span><span class="si">{:.3f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;eig_mean&#39;</span><span class="p">][</span><span class="nb">iter</span><span class="p">])</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">][</span><span class="nb">iter</span><span class="p">]):</span>
        <span class="n">msg</span> <span class="o">+=</span> <span class="s1">&#39;, r=</span><span class="si">{:.5f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">][</span><span class="nb">iter</span><span class="p">])</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;violations&#39;</span><span class="p">][</span><span class="nb">iter</span><span class="p">]):</span>
        <span class="n">msg</span> <span class="o">+=</span> <span class="s1">&#39;, v=</span><span class="si">{:.0f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;violations&#39;</span><span class="p">][</span><span class="nb">iter</span><span class="p">])</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="LEMBAS.utilities.set_cores" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">set_cores</span><span class="p">(</span><span class="n">n_cores</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Set environmental variables to ensure core usage is limited to n_cores</p>
<h5 id="LEMBAS.utilities.set_cores--parameters">Parameters</h5>
<p>n_cores : int
    number of cores to use</p>

          <details class="quote">
            <summary>Source code in <code>LEMBAS/utilities.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_cores</span><span class="p">(</span><span class="n">n_cores</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Set environmental variables to ensure core usage is limited to n_cores</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_cores : int</span>
<span class="sd">        number of cores to use</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OMP_NUM_THREADS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_cores</span><span class="p">)</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MKL_NUM_THREADS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_cores</span><span class="p">)</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OPENBLAS_NUM_THREADS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_cores</span><span class="p">)</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;VECLIB_MAXIMUM_THREADS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_cores</span><span class="p">)</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;NUMEXPR_NUM_THREADS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_cores</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="LEMBAS.utilities.set_seeds" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">set_seeds</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">888</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Sets random seeds for torch operations.</p>
<h5 id="LEMBAS.utilities.set_seeds--parameters">Parameters</h5>
<p>seed : int, optional
    seed value, by default 888</p>

          <details class="quote">
            <summary>Source code in <code>LEMBAS/utilities.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_seeds</span><span class="p">(</span><span class="n">seed</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">888</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sets random seeds for torch operations.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        seed value, by default 888</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="s1">&#39;CUBLAS_WORKSPACE_CONFIG&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;CUBLAS_WORKSPACE_CONFIG&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;:4096:8&#39;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="LEMBAS.utilities.update_progress" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">update_progress</span><span class="p">(</span><span class="n">stats</span><span class="p">,</span> <span class="nb">iter</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eig</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_sign_mismatches</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Updates various stats of the progress of training the model.</p>
<h5 id="LEMBAS.utilities.update_progress--parameters">Parameters</h5>
<p>stats : dict
    a dictionary of progress statistics
iter : int
    the current training iteration
loss : List[float], optional
    a list of the loss (excluding regularizations) up to <code>iter</code> , by default None
eig : List[float], optional
    a list of the spectral_radius up to <code>iter</code> , by default None
learning_rate : float, optional
    the model learning rate at <code>iter</code>, by default None
n_sign_mismatches : float, optional
    the total number of sign mismatches at <code>iter</code>, 
    output of <code>SignalingModel.signaling_network.count_sign_mismatch()</code>, by default None</p>
<h5 id="LEMBAS.utilities.update_progress--returns">Returns</h5>
<p>stats : dict
    updated dictionary of progress statistics</p>

          <details class="quote">
            <summary>Source code in <code>LEMBAS/utilities.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">update_progress</span><span class="p">(</span><span class="n">stats</span> <span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="nb">iter</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> 
                  <span class="n">loss</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eig</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                  <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_sign_mismatches</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Updates various stats of the progress of training the model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    stats : dict</span>
<span class="sd">        a dictionary of progress statistics</span>
<span class="sd">    iter : int</span>
<span class="sd">        the current training iteration</span>
<span class="sd">    loss : List[float], optional</span>
<span class="sd">        a list of the loss (excluding regularizations) up to `iter` , by default None</span>
<span class="sd">    eig : List[float], optional</span>
<span class="sd">        a list of the spectral_radius up to `iter` , by default None</span>
<span class="sd">    learning_rate : float, optional</span>
<span class="sd">        the model learning rate at `iter`, by default None</span>
<span class="sd">    n_sign_mismatches : float, optional</span>
<span class="sd">        the total number of sign mismatches at `iter`, </span>
<span class="sd">        output of `SignalingModel.signaling_network.count_sign_mismatch()`, by default None</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    stats : dict</span>
<span class="sd">        updated dictionary of progress statistics</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">loss</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;loss_mean&#39;</span><span class="p">][</span><span class="nb">iter</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">loss</span><span class="p">))</span>
        <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;loss_sigma&#39;</span><span class="p">][</span><span class="nb">iter</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">loss</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">eig</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;eig_mean&#39;</span><span class="p">][</span><span class="nb">iter</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">eig</span><span class="p">))</span>
        <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;eig_sigma&#39;</span><span class="p">][</span><span class="nb">iter</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">eig</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">learning_rate</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">][</span><span class="nb">iter</span><span class="p">]</span> <span class="o">=</span> <span class="n">learning_rate</span>
    <span class="k">if</span> <span class="n">n_sign_mismatches</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;violations&#39;</span><span class="p">][</span><span class="nb">iter</span><span class="p">]</span> <span class="o">=</span> <span class="n">n_sign_mismatches</span>

    <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;iter_time&#39;</span><span class="p">][</span><span class="nb">iter</span><span class="p">]</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">stats</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>


  </div>

  </div>

</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../macrophage_example/" class="btn btn-neutral float-left" title="Build the Model"><span class="icon icon-circle-arrow-left"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../macrophage_example/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
