{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"scLEMBAS Python package for mecahnistically modeling ligand responses at single-cell resolution.","title":"Home"},{"location":"#sclembas","text":"Python package for mecahnistically modeling ligand responses at single-cell resolution.","title":"scLEMBAS"},{"location":"api/","text":"io Read and write python objects. read_pickled_object ( file_name ) Read an object as a pickled file. Parameters file_name : str 'full/path/to/file.pickle' Returns pickled_object the pickled object Source code in scLEMBAS/io.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def read_pickled_object ( file_name : str ): \"\"\"Read an object as a pickled file. Parameters ---------- file_name : str 'full/path/to/file.pickle' Returns ------- pickled_object the pickled object \"\"\" with open ( file_name , 'rb' ) as handle : pickled_object = pickle . load ( handle ) return pickled_object write_pickled_object ( object , file_name ) Save an object as a pickled file. Parameters object : Any object to save file_name : str 'full/path/to/file.pickle' Source code in scLEMBAS/io.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def write_pickled_object ( object : Any , file_name : str ) -> None : \"\"\"Save an object as a pickled file. Parameters ---------- object : Any object to save file_name : str 'full/path/to/file.pickle' \"\"\" if '.' in file_name : p = pathlib . Path ( file_name ) extensions = \"\" . join ( p . suffixes ) file_name = str ( p ) . replace ( extensions , '.pickle' ) else : file_name = file_name + '.pickle' with open ( file_name , 'wb' ) as handle : pickle . dump ( object , handle ) model activation_functions Defines various activations functions to be used in neural net. MML_activation ( x , leak ) Returns the output of the Michaelis-Menten function Parameters x : torch.Tensor a vector of input values leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU Returns fx : torch.Tensor a vector of output values Source code in scLEMBAS/model/activation_functions.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def MML_activation ( x : torch . Tensor , leak : Union [ float , int ]): \"\"\"Returns the output of the Michaelis-Menten function Parameters ---------- x : torch.Tensor a vector of input values leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU Returns ------- fx : torch.Tensor a vector of output values \"\"\" fx = torch . nn . functional . leaky_relu ( input = x , negative_slope = leak , inplace = False ) shifted_x = 0.5 * ( fx - 0.5 ) mask = torch . lt ( shifted_x , 0.0 ) gated_x = fx + 10 * mask #prevents division by 0 issue on next line right_values = 0.5 + torch . div ( shifted_x , gated_x ) fx = mask * ( fx - right_values ) + right_values #-fx trick from relu return fx MML_delta_activation ( x , leak ) Returns the derivative of the Michaelis-Menten function Parameters x : torch.Tensor a vector of input values leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU Returns y : torch.Tensor a vector of output derivative values Source code in scLEMBAS/model/activation_functions.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def MML_delta_activation ( x : torch . Tensor , leak : Union [ float , int ]): \"\"\"Returns the derivative of the Michaelis-Menten function Parameters ---------- x : torch.Tensor a vector of input values leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU Returns ------- y : torch.Tensor a vector of output derivative values \"\"\" mask1 = x . le ( 0 ) y = torch . ones ( x . shape , dtype = x . dtype , device = x . device ) #derivative = 1 if nothing else is stated mask2 = x . gt ( 0.5 ) right_values = 0.25 / torch . pow ( x + 1e-12 , 2 ) - 1 # add psuedocount bc x = 0 will creat NaN y = y + mask2 * right_values y = y - ( 1 - leak ) * mask1 return y MML_onestepdelta_activation_factor ( Y_full , leak = 0.01 ) Adjusts weights for linearization in the spectral radius. Note that this will only work for monotonic functions Parameters Y_full : torch.Tensor description leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01 Returns y : torch.Tensor description Source code in scLEMBAS/model/activation_functions.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def MML_onestepdelta_activation_factor ( Y_full : torch . Tensor , leak : Union [ float , int ] = 0.01 ): \"\"\"Adjusts weights for linearization in the spectral radius. Note that this will only work for monotonic functions Parameters ---------- Y_full : torch.Tensor _description_ leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01 Returns ------- y : torch.Tensor _description_ \"\"\" y = torch . ones_like ( Y_full ) piece1 = Y_full . le ( 0 ) piece3 = Y_full . gt ( 0.5 ) safe_x = torch . clamp ( 1 - Y_full , max = 0.9999 ) right_values = 4 * torch . pow ( safe_x , 2 ) - 1 y = y + piece3 * right_values y = y - ( 1 - leak ) * piece1 return y leakyReLU_activation ( x , leak = 0.01 ) Returns the output of the leaky ReLU function Parameters x : Iterable[Union[float, int]] a vector of input values leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01 Returns fx a vector of output values Source code in scLEMBAS/model/activation_functions.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def leakyReLU_activation ( x , leak = 0.01 ): \"\"\"Returns the output of the leaky ReLU function Parameters ---------- x : Iterable[Union[float, int]] a vector of input values leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01 Returns ------- fx a vector of output values \"\"\" fx = np . copy ( x ) fx = np . where ( fx < 0 , fx * leak , fx ) return fx leakyReLU_delta_activation ( x , leak = 0.01 ) Returns the derivative of the leaky ReLU function Parameters x : Iterable[Union[float, int]] a vector of input values leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01 Returns y : Iterable[Union[float, int]] a vector of output derivative values Source code in scLEMBAS/model/activation_functions.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def leakyReLU_delta_activation ( x , leak = 0.01 ): \"\"\"Returns the derivative of the leaky ReLU function Parameters ---------- x : Iterable[Union[float, int]] a vector of input values leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01 Returns ------- y : Iterable[Union[float, int]] a vector of output derivative values \"\"\" y = np . ones ( x . shape ) #derivative = 1 if nothing else is stated y = np . where ( x <= 0 , leak , y ) #let derivative be 0.01 at x=0 return y sigmoid_activation ( x , leak = 0 ) Returns the output of the sigmoid function Parameters x : Iterable[Union[float, int]] a vector of input values leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01 Returns fx a vector of output values Source code in scLEMBAS/model/activation_functions.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def sigmoid_activation ( x , leak = 0 ): \"\"\"Returns the output of the sigmoid function Parameters ---------- x : Iterable[Union[float, int]] a vector of input values leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01 Returns ------- fx a vector of output values \"\"\" fx = np . copy ( x ) #leak is not used for sigmoid_ fx = 1 / ( 1 + np . exp ( - fx )) return fx sigmoid_delta_activation ( x , leak = 0 ) Returns the derivative of the sigmoid function Parameters x : Iterable[Union[float, int]] a vector of input values leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01 Returns y : Iterable[Union[float, int]] a vector of output derivative values Source code in scLEMBAS/model/activation_functions.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def sigmoid_delta_activation ( x , leak = 0 ): \"\"\"Returns the derivative of the sigmoid function Parameters ---------- x : Iterable[Union[float, int]] a vector of input values leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01 Returns ------- y : Iterable[Union[float, int]] a vector of output derivative values \"\"\" y = sigmoid_activation ( x ) * ( 1 - sigmoid_activation ( x )) return y model_utilities Helper functions for building the model. format_network ( net , weight_label = 'mode_of_action' , stimulation_label = 'stimulation' , inhibition_label = 'inhibition' ) Formats the standard network file format to that needed by SignalingModel.parse_network Parameters net : pd.DataFrame signaling network adjacency list with the following columns: - weight_label : whether the interaction is stimulating (1) or inhibiting (-1) or unknown (0.1). Exclude non-interacting (0) nodes. - stimulation_label : binary whether an interaction is stimulating (1) or [not stimultaing or unknown] (0) - inhibition_label : binary whether an interaction is inhibiting (1) or [not inhibiting or unknown] (0) weight_label : str, optional converts stimulation_label and inhibition_label to a single column of stimulating (1), inhibiting (-1), or unknown (0.1), by default 'mode_of_action' stimulation_label : str, optional column name of stimulating interactions, see net , by default 'stimulation' inhibition_label : str, optional column name of inhibitory interactions, see net , by default 'inhibition' Returns formatted_net : pd.DataFrame the same dataframe with the additional weight_label column Source code in scLEMBAS/model/model_utilities.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def format_network ( net : pd . DataFrame , weight_label : str = 'mode_of_action' , stimulation_label : str = 'stimulation' , inhibition_label : str = 'inhibition' ) -> pd . DataFrame : \"\"\"Formats the standard network file format to that needed by `SignalingModel.parse_network` Parameters ---------- net : pd.DataFrame signaling network adjacency list with the following columns: - `weight_label`: whether the interaction is stimulating (1) or inhibiting (-1) or unknown (0.1). Exclude non-interacting (0) nodes. - `stimulation_label`: binary whether an interaction is stimulating (1) or [not stimultaing or unknown] (0) - `inhibition_label`: binary whether an interaction is inhibiting (1) or [not inhibiting or unknown] (0) weight_label : str, optional converts `stimulation_label` and `inhibition_label` to a single column of stimulating (1), inhibiting (-1), or unknown (0.1), by default 'mode_of_action' stimulation_label : str, optional column name of stimulating interactions, see `net`, by default 'stimulation' inhibition_label : str, optional column name of inhibitory interactions, see `net`, by default 'inhibition' Returns ------- formatted_net : pd.DataFrame the same dataframe with the additional `weight_label` column \"\"\" if net [( net [ stimulation_label ] == 1 ) & ( net [ inhibition_label ] == 1 )] . shape [ 0 ] > 0 : raise ValueError ( 'An interaction can either be stimulating (1,0), inhibition (0,1) or unknown (0,0)' ) formatted_net = net . copy () formatted_net [ weight_label ] = np . zeros ( net . shape [ 0 ]) formatted_net . loc [ formatted_net [ stimulation_label ] == 1 , weight_label ] = 1 formatted_net . loc [ formatted_net [ inhibition_label ] == 1 , weight_label ] = - 1 #ensuring that lack of known MOA does not imply lack of representation in scipy.sparse.find(A) formatted_net [ weight_label ] = formatted_net [ weight_label ] . replace ( 0 , 0.1 ) formatted_net [ weight_label ] = formatted_net [ weight_label ] . replace ( np . nan , 0.1 ) return formatted_net np_to_torch ( arr , dtype , device = 'cpu' ) Convert a numpy array to a torch.tensor Parameters arr : np.array torch.dtype, optional datatype to store values in torch, by default torch.float32 device : str whether to use gpu (\"cuda\") or cpu (\"cpu\"), by default \"cpu\" Source code in scLEMBAS/model/model_utilities.py 10 11 12 13 14 15 16 17 18 19 20 21 22 def np_to_torch ( arr : np . array , dtype : torch . float32 , device : str = 'cpu' ): \"\"\"Convert a numpy array to a torch.tensor Parameters ---------- arr : np.array dtype : torch.dtype, optional datatype to store values in torch, by default torch.float32 device : str whether to use gpu (\"cuda\") or cpu (\"cpu\"), by default \"cpu\" \"\"\" return torch . tensor ( arr , dtype = dtype , device = device ) train Train the signaling model. ModelData Bases: Dataset Source code in scLEMBAS/model/train.py 65 66 67 68 69 70 71 72 73 74 class ModelData ( Dataset ): def __init__ ( self , X_in , y_out ): self . X_in = X_in self . y_out = y_out def __len__ ( self ) -> int : \"Returns the total number of samples.\" return self . X_in . shape [ 0 ] def __getitem__ ( self , idx : int ): \"Returns one sample of data, data and label (X, y).\" return self . X_in [ idx , :], self . y_out [ idx , :] __getitem__ ( idx ) Returns one sample of data, data and label (X, y). Source code in scLEMBAS/model/train.py 72 73 74 def __getitem__ ( self , idx : int ): \"Returns one sample of data, data and label (X, y).\" return self . X_in [ idx , :], self . y_out [ idx , :] __len__ () Returns the total number of samples. Source code in scLEMBAS/model/train.py 69 70 71 def __len__ ( self ) -> int : \"Returns the total number of samples.\" return self . X_in . shape [ 0 ] split_data ( X_in , y_out , train_split_frac = { 'train' : 0.8 , 'test' : 0.2 , 'validation' : None }, seed = 888 ) Splits the data into train, test, and validation. Parameters X_in : torch.Tensor input ligand concentrations. Index represents samples and columns represent a ligand. Values represent amount of ligand introduced (e.g., concentration). y_out : torch.Tensor output TF activities. Index represents samples and columns represent TFs. Values represent activity of the TF. train_split_frac : Dict, optional fraction of samples to be assigned to each of train, test and split, by default 0.8, 0.2, and 0 respectively seed : int, optional seed value, by default 888 Source code in scLEMBAS/model/train.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def split_data ( X_in : torch . Tensor , y_out : torch . Tensor , train_split_frac : Dict = { 'train' : 0.8 , 'test' : 0.2 , 'validation' : None }, seed : int = 888 ): \"\"\"Splits the data into train, test, and validation. Parameters ---------- X_in : torch.Tensor input ligand concentrations. Index represents samples and columns represent a ligand. Values represent amount of ligand introduced (e.g., concentration). y_out : torch.Tensor output TF activities. Index represents samples and columns represent TFs. Values represent activity of the TF. train_split_frac : Dict, optional fraction of samples to be assigned to each of train, test and split, by default 0.8, 0.2, and 0 respectively seed : int, optional seed value, by default 888 \"\"\" if not np . isclose ( sum ([ v for v in train_split_frac . values () if v ]), 1 ): raise ValueError ( 'Train-test-validation split must sum to 1' ) if not train_split_frac [ 'validation' ] or train_split_frac [ 'validation' ] == 0 : X_train , X_test , y_train , y_test = train_test_split ( X_in , y_out , train_size = train_split_frac [ 'train' ], random_state = seed ) X_val , y_val = None , None else : X_train , _X , y_train , _y = train_test_split ( X_in , y_out , train_size = train_split_frac [ 'train' ], random_state = seed ) X_test , X_val , y_test , y_val = train_test_split ( _X , _y , train_size = train_split_frac [ 'test' ] / ( train_split_frac [ 'test' ] + train_split_frac [ 'validation' ]), random_state = seed ) return X_train , X_test , X_val , y_train , y_test , y_val train_signaling_model ( mod , optimizer , loss_fn , reset_epoch = 200 , hyper_params = None , train_split_frac = { 'train' : 0.8 , 'test' : 0.2 , 'validation' : None }, train_seed = None , verbose = True ) Trains the signaling model Parameters mod : SignalingModel initialized signaling model. Suggested to also run mod.signaling_network.prescale_weights prior to training optimizer : torch.optim.adam.Adam optimizer to use during training loss_fn : torch.nn.modules.loss.MSELoss loss function to use during training reset_epoch : int, optional number of epochs upon which to reset the optimizer state, by default 200 hyper_params : Dict[str, Union[int, float]], optional various hyper parameter inputs for training - 'max_iter' : the number of epochs, by default 5000 - 'learning_rate' : the starting learning rate, by default 2e-3 - 'batch_size' : number of samples per batch, by default 8 - 'noise_level' : noise added to signaling network input, by default 10. Set to 0 for no noise. Makes model more robust. - 'gradient_noise_level' : noise added to gradient after backward pass. Makes model more robust. - 'reset_epoch' : number of epochs upon which to reset the optimizer state, by default 200 - 'param_lambda_L2' : L2 regularization penalty term for most of the model weights and biases - 'moa_lambda_L1' : L1 regularization penalty term for incorrect interaction mechanism of action (inhibiting/stimulating) - 'ligand_lambda_L2' : L2 regularization penalty term for ligand biases - 'uniform_lambda_L2' : L2 regularization penalty term for - 'uniform_max' : - 'spectral_loss_factor' : regularization penalty term for - 'n_probes_spectral' : - 'power_steps_spectral' : - 'subset_n_spectral' : train_split_frac : Dict, optional fraction of samples to be assigned to each of train, test and split, by default 0.8, 0.2, and 0 respectively train_seed : int, optional seed value, by default mod.seed. By explicitly making this an argument, it allows different train-test splits even with the same mod.seed, e.g., for cross-validation verbose : bool, optional whether to print various progress stats across training epochs Returns mod : SignalingModel a copy of the input model with trained parameters cur_loss : List[float], optional a list of the loss (excluding regularizations) across training iterations cur_eig : List[float], optional a list of the spectral_radius across training iterations mean_loss : torch.Tensor mean TF activity loss across samples (independent of training) X_train : torch.Tensor the train split of the input data X_test : torch.Tensor the test split of the input data X_val : torch.Tensor the validation split of the input data y_train : torch.Tensor the train split of the output data y_test : torch.Tensor the test split of the output data y_val : torch.Tensor the validation split of the output data Source code in scLEMBAS/model/train.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def train_signaling_model ( mod , optimizer : torch . optim , loss_fn : torch . nn . modules . loss , reset_epoch : int = 200 , hyper_params : Dict [ str , Union [ int , float ]] = None , train_split_frac : Dict = { 'train' : 0.8 , 'test' : 0.2 , 'validation' : None }, train_seed : int = None , verbose : bool = True ): \"\"\"Trains the signaling model Parameters ---------- mod : SignalingModel initialized signaling model. Suggested to also run `mod.signaling_network.prescale_weights` prior to training optimizer : torch.optim.adam.Adam optimizer to use during training loss_fn : torch.nn.modules.loss.MSELoss loss function to use during training reset_epoch : int, optional number of epochs upon which to reset the optimizer state, by default 200 hyper_params : Dict[str, Union[int, float]], optional various hyper parameter inputs for training - 'max_iter' : the number of epochs, by default 5000 - 'learning_rate' : the starting learning rate, by default 2e-3 - 'batch_size' : number of samples per batch, by default 8 - 'noise_level' : noise added to signaling network input, by default 10. Set to 0 for no noise. Makes model more robust. - 'gradient_noise_level' : noise added to gradient after backward pass. Makes model more robust. - 'reset_epoch' : number of epochs upon which to reset the optimizer state, by default 200 - 'param_lambda_L2' : L2 regularization penalty term for most of the model weights and biases - 'moa_lambda_L1' : L1 regularization penalty term for incorrect interaction mechanism of action (inhibiting/stimulating) - 'ligand_lambda_L2' : L2 regularization penalty term for ligand biases - 'uniform_lambda_L2' : L2 regularization penalty term for - 'uniform_max' : - 'spectral_loss_factor' : regularization penalty term for - 'n_probes_spectral' : - 'power_steps_spectral' : - 'subset_n_spectral' : train_split_frac : Dict, optional fraction of samples to be assigned to each of train, test and split, by default 0.8, 0.2, and 0 respectively train_seed : int, optional seed value, by default mod.seed. By explicitly making this an argument, it allows different train-test splits even with the same mod.seed, e.g., for cross-validation verbose : bool, optional whether to print various progress stats across training epochs Returns ------- mod : SignalingModel a copy of the input model with trained parameters cur_loss : List[float], optional a list of the loss (excluding regularizations) across training iterations cur_eig : List[float], optional a list of the spectral_radius across training iterations mean_loss : torch.Tensor mean TF activity loss across samples (independent of training) X_train : torch.Tensor the train split of the input data X_test : torch.Tensor the test split of the input data X_val : torch.Tensor the validation split of the input data y_train : torch.Tensor the train split of the output data y_test : torch.Tensor the test split of the output data y_val : torch.Tensor the validation split of the output data \"\"\" if not hyper_params : hyper_params = HYPER_PARAMS . copy () else : hyper_params = { k : v for k , v in { ** HYPER_PARAMS , ** hyper_params } . items () if k in HYPER_PARAMS } # give user input priority stats = utils . initialize_progress ( hyper_params [ 'max_iter' ]) mod = mod . copy () # do not overwrite input optimizer = optimizer ( mod . parameters (), lr = 1 , weight_decay = 0 ) reset_state = optimizer . state . copy () X_in = mod . df_to_tensor ( mod . X_in ) y_out = mod . df_to_tensor ( mod . y_out ) mean_loss = loss_fn ( torch . mean ( y_out , dim = 0 ) * torch . ones ( y_out . shape , device = y_out . device ), y_out ) # mean TF (across samples) loss # set up data objects if not train_seed : train_seed = mod . seed X_train , X_test , X_val , y_train , y_test , y_val = split_data ( X_in , y_out , train_split_frac , train_seed ) train_data = ModelData ( X_train . to ( 'cpu' ), y_train . to ( 'cpu' )) if mod . device == 'cuda' : pin_memory = True else : pin_memory = False # if n_cores != 0: # n_cores_train = min(n_cores, hyper_params['batch_size']) # else: # n_cores_train = n_cores train_dataloader = DataLoader ( dataset = train_data , batch_size = hyper_params [ 'batch_size' ], # num_workers=n_cores_train, drop_last = False , pin_memory = pin_memory , shuffle = True ) start_time = time . time () # begin iteration for e in range ( hyper_params [ 'max_iter' ]): # set learning rate cur_lr = utils . get_lr ( e , hyper_params [ 'max_iter' ], max_height = hyper_params [ 'learning_rate' ], start_height = hyper_params [ 'learning_rate' ] / 10 , end_height = 1e-6 , peak = 1000 ) optimizer . param_groups [ 0 ][ 'lr' ] = cur_lr cur_loss = [] cur_eig = [] # iterate through batches if mod . seed : utils . set_seeds ( mod . seed + e ) for batch , ( X_in_ , y_out_ ) in enumerate ( train_dataloader ): mod . train () optimizer . zero_grad () X_in_ , y_out_ = X_in_ . to ( mod . device ), y_out_ . to ( mod . device ) # forward pass X_full = mod . input_layer ( X_in_ ) # transform to full network with ligand input concentrations utils . set_seeds ( mod . seed + mod . _gradient_seed_counter ) network_noise = torch . randn ( X_full . shape , device = X_full . device ) X_full = X_full + ( hyper_params [ 'noise_level' ] * cur_lr * network_noise ) # randomly add noise to signaling network input, makes model more robust Y_full = mod . signaling_network ( X_full ) # train signaling network weights Y_hat = mod . output_layer ( Y_full ) # get prediction loss fit_loss = loss_fn ( y_out_ , Y_hat ) # get regularization losses sign_reg = mod . signaling_network . sign_regularization ( lambda_L1 = hyper_params [ 'moa_lambda_L1' ]) # incorrect MoA ligand_reg = mod . ligand_regularization ( lambda_L2 = hyper_params [ 'ligand_lambda_L2' ]) # ligand biases stability_loss , spectral_radius = mod . signaling_network . get_SS_loss ( Y_full = Y_full . detach (), spectral_loss_factor = hyper_params [ 'spectral_loss_factor' ], subset_n = hyper_params [ 'subset_n_spectral' ], n_probes = hyper_params [ 'n_probes_spectral' ], power_steps = hyper_params [ 'power_steps_spectral' ]) uniform_reg = mod . uniform_regularization ( lambda_L2 = hyper_params [ 'uniform_lambda_L2' ] * cur_lr , Y_full = Y_full , target_min = 0 , target_max = hyper_params [ 'uniform_max' ]) # uniform distribution param_reg = mod . L2_reg ( hyper_params [ 'param_lambda_L2' ]) # all model weights and signaling network biases total_loss = fit_loss + sign_reg + ligand_reg + param_reg + stability_loss + uniform_reg # gradient total_loss . backward () mod . add_gradient_noise ( noise_level = hyper_params [ 'gradient_noise_level' ]) optimizer . step () # store cur_eig . append ( spectral_radius ) cur_loss . append ( fit_loss . item ()) stats = utils . update_progress ( stats , iter = e , loss = cur_loss , eig = cur_eig , learning_rate = cur_lr , n_sign_mismatches = mod . signaling_network . count_sign_mismatch ()) if verbose and e % 250 == 0 : utils . print_stats ( stats , iter = e ) if np . logical_and ( e % reset_epoch == 0 , e > 0 ): optimizer . state = reset_state . copy () if verbose : mins , secs = divmod ( time . time () - start_time , 60 ) print ( \"Training ran in: {:.0f} min {:.2f} sec\" . format ( mins , secs )) return mod , cur_loss , cur_eig , mean_loss , stats , X_train , X_test , X_val , y_train , y_test , y_val plotting Helper functions for data visualization. shade_plot ( X , Y , sigma , x_label , y_label , width = 5 , height = 3 ) summary Parameters X : np.array x axis values Y : np.array y axis values sigma : np.array standard deviation of y axis values x_label : str x axis label y_label : str y axis label Returns plot : plotnine.ggplot.ggplot description Source code in scLEMBAS/plotting.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def shade_plot ( X : np . array , Y : np . array , sigma : np . array , x_label : str , y_label : str , width : int = 5 , height : int = 3 ): \"\"\"_summary_ Parameters ---------- X : np.array x axis values Y : np.array y axis values sigma : np.array standard deviation of y axis values x_label : str x axis label y_label : str y axis label Returns ------- plot : plotnine.ggplot.ggplot _description_ \"\"\" data = pd . DataFrame ( data = { x_label : X , y_label : Y , 'sigma' : sigma }) data [ 'sigma_min' ] = data [ y_label ] - data . sigma data [ 'sigma_max' ] = data [ y_label ] + data . sigma plot = ( p9 . ggplot ( data , p9 . aes ( x = x_label )) + p9 . geom_line ( p9 . aes ( y = y_label ), color = '#1E90FF' ) + p9 . geom_ribbon ( p9 . aes ( y = y_label , ymin = 'sigma_min' , ymax = 'sigma_max' ), alpha = 0.2 ) + p9 . xlim ([ 0 , data . shape [ 0 ]]) + # p9.ylim(10**min_log, round(data[y_label].max(), 1)) + # p9.scale_y_log10() + p9 . theme_bw () + p9 . theme ( figure_size = ( width , height )) ) return plot preprocess Preprocessing functions for single-cell AnnData objects. get_tf_activity ( adata , organism , grn = 'collectri' , verbose = True , min_n = 5 , use_raw = False , filter_pvals = False , pval_thresh = 0.05 , ** kwargs ) Wrapper of decoupler to estimate TF activity from single-cell transcriptomics data. Parameters adata : AnnData Annotated single-cell data matrix organism : str The organism of interest: either NCBI Taxonomy ID, common name, latin name or Ensembl name. Organisms other than human will be translated from human data by orthology. grn : str, optional database to get the GRN, by default 'collectri'. Available options are collectri or dorothea . min_n : int Minimum of targets per source. If less, sources are removed. By default 5. verbose : bool Whether to show progress. use_raw : bool Use AnnData its raw attribute. filter_pvals : bool whether to set TF activity estimates to 0 if it is insignificant according to pval_thresh pval_thresh : float significance threshold, by default 0.05. Used in conjunction with filter_pvals. kwargs : passed to decoupler.decouple . Returns estimate : DataFrame Consensus TF activity scores. Stored in .obsm['consensus_estimate'] . pvals : DataFrame Obtained TF activity p-values. Stored in .obsm['consensus_pvals'] . Source code in scLEMBAS/preprocess.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def get_tf_activity ( adata , organism : str , grn = 'collectri' , verbose : bool = True , min_n : int = 5 , use_raw : bool = False , filter_pvals : bool = False , pval_thresh : float = 0.05 , ** kwargs ): \"\"\"Wrapper of decoupler to estimate TF activity from single-cell transcriptomics data. Parameters ---------- adata : AnnData Annotated single-cell data matrix organism : str The organism of interest: either NCBI Taxonomy ID, common name, latin name or Ensembl name. Organisms other than human will be translated from human data by orthology. grn : str, optional database to get the GRN, by default 'collectri'. Available options are ``collectri`` or ``dorothea``. min_n : int Minimum of targets per source. If less, sources are removed. By default 5. verbose : bool Whether to show progress. use_raw : bool Use AnnData its raw attribute. filter_pvals : bool whether to set TF activity estimates to 0 if it is insignificant according to pval_thresh pval_thresh : float significance threshold, by default 0.05. Used in conjunction with filter_pvals. kwargs : passed to `decoupler.decouple`. Returns ------- estimate : DataFrame Consensus TF activity scores. Stored in `.obsm['consensus_estimate']`. pvals : DataFrame Obtained TF activity p-values. Stored in `.obsm['consensus_pvals']`. \"\"\" grn_map = { 'collectri' : dc . get_collectri , 'dorothea' : dc . get_dorothea } # get_dorothea returns \"A\" confidence by default net = grn_map [ grn ]( organism = organism , split_complexes = False ) # builds on dorothea, used by Saez-Rodriguez lab # # reimplementation of dc.run_consensus, allowing all options in dc.decouple to be passed # dc.run_consensus(mat=adata, net=net, source='source', target='target', weight='weight', **kwargs) # m, r, c = extract(adata, use_raw=use_raw, verbose=verbose) if verbose : print ( 'Running consensus.' ) # # unnecessary, this is the default behavior # if not kwargs: # kwargs = {'methods': ['lm', 'ulm', 'wsum'], # 'cns_metds': ['lm', 'ulm', 'wsum_norm']} # else: # if 'methods' not in kwargs: # kwargs['methods'] = ['lm', 'ulm', 'wsum'] # if 'cns_methods' not in kwargs and kwargs['methods'] == ['lm', 'ulm', 'wsum']: # kwargs['cns_metds'] = ['lm', 'ulm', 'wsum_norm'] dc . decouple ( mat = adata , net = net , source = 'source' , target = 'target' , weight = 'weight' , consensus = True , min_n = min_n , verbose = verbose , use_raw = use_raw , ** kwargs ) if filter_pvals : estimate_key = [ k for k in adata . obsm if k . endswith ( '_estimate' )] pvals_key = [ k for k in adata . obsm if k . endswith ( '_pvals' )] if len ( estimate_key ) > 0 or len ( pvals_key ) > 0 : warnings . warn ( 'Multiple TF estimates/pvals, choosing first' ) adata . obsm [ estimate_key [ 0 ]][ adata . obsm [ pvals_key [ 0 ]] > pval_thresh ] = 0 project_tf_activity ( adata , estimate_key = 'consensus_estimate' ) Runs dimensionality reduction and clustering of cells from their TF activity using default scanpy parameters. Parameters adata : AnnData AnnData object with TF activity scores stored in .obsm[estimate_key] . estimate_key : str, optional .obsm key under which TF activity is stored, by default 'consensus_estimate' Returns tf_adata : AnnData AnnData object with input TF activity estimates stored in .X , and dimensionality reduction and clustering outputs stored in default scanpy locations. Cluster labels on TF activity space are stores in adata.obs['TF_clusters'] Source code in scLEMBAS/preprocess.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def project_tf_activity ( adata : AnnData , estimate_key : str = 'consensus_estimate' ): \"\"\"Runs dimensionality reduction and clustering of cells from their TF activity using default scanpy parameters. Parameters ---------- adata : AnnData AnnData object with TF activity scores stored in `.obsm[estimate_key]`. estimate_key : str, optional `.obsm` key under which TF activity is stored, by default 'consensus_estimate' Returns ------- tf_adata : AnnData AnnData object with input TF activity estimates stored in `.X`, and dimensionality reduction and clustering outputs stored in default scanpy locations. Cluster labels on TF activity space are stores in `adata.obs['TF_clusters']` \"\"\" tf_adata = AnnData ( adata . obsm [ estimate_key ]) sc . tl . pca ( data = tf_adata ) # _pca_simple(adata = tf_adata) # run PCA # only needed for the model to use transform, but sc.tl.ingest can do this pc_rank = _compute_elbow ( adata = tf_adata ) tf_adata . uns [ \"pca\" ][ 'pca_rank' ] = pc_rank # if not np.allclose(tf_adata.obsm['X_pca'], tf_adata.uns['pca']['pca_mod'].transform(tf_adata.X)): # raise ValueError('Unexpected disagreement when running PCA.transform') sc . pp . neighbors ( adata = tf_adata , n_pcs = pc_rank ) # construct neighborhood graph sc . tl . umap ( adata = tf_adata ) # run UMAP sc . tl . leiden ( adata = tf_adata ) # cluster tf_adata . obs . rename ( columns = { 'leiden' : 'TF_clusters' }, inplace = True ) tf_adata . obs = pd . concat ([ adata . obs , tf_adata . obs ], axis = 1 ) return tf_adata utilities Helper functions for running and training the SignalingModel. get_lr ( iter , max_iter , max_height = 0.001 , start_height = 1e-05 , end_height = 1e-05 , peak = 1000 ) Calculates learning rate for a given iteration during training. Parameters iter : int the current iteration max_iter : int the maximum number of training iterations max_height : float, optional tuning parameters for learning for the first 95% of iterations, by default 1e-3 start_height : float, optional tuning parameter for learning rate before peak iterations, by default 1e-5 end_height : float, optional tuning parameter for learning rate afer peak iterations, by default 1e-5 peak : int, optional the first # of iterations to calculate lr on (should be less than 95% of max_iter), by default 1000 Returns lr : float the learning rate Source code in scLEMBAS/utilities.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def get_lr ( iter : int , max_iter : int , max_height : float = 1e-3 , start_height : float = 1e-5 , end_height : float = 1e-5 , peak : int = 1000 ): \"\"\"Calculates learning rate for a given iteration during training. Parameters ---------- iter : int the current iteration max_iter : int the maximum number of training iterations max_height : float, optional tuning parameters for learning for the first 95% of iterations, by default 1e-3 start_height : float, optional tuning parameter for learning rate before peak iterations, by default 1e-5 end_height : float, optional tuning parameter for learning rate afer peak iterations, by default 1e-5 peak : int, optional the first # of iterations to calculate lr on (should be less than 95% of max_iter), by default 1000 Returns ------- lr : float the learning rate \"\"\" phase_length = 0.95 * max_iter if iter <= peak : effective_iter = iter / peak lr = ( max_height - start_height ) * 0.5 * ( np . cos ( np . pi * ( effective_iter + 1 )) + 1 ) + start_height elif iter <= phase_length : effective_iter = ( iter - peak ) / ( phase_length - peak ) lr = ( max_height - end_height ) * 0.5 * ( np . cos ( np . pi * ( effective_iter + 2 )) + 1 ) + end_height else : lr = end_height return lr get_moving_average ( values , n_steps ) Get the moving average of a tracked state across n_steps. Serves to smooth value. Parameters values : np.array values on which to get the moving average n_steps : int number of steps across which to get the moving average Returns moving_average : np.array the moving average across values Source code in scLEMBAS/utilities.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def get_moving_average ( values : np . array , n_steps : int ): \"\"\"Get the moving average of a tracked state across n_steps. Serves to smooth value. Parameters ---------- values : np.array values on which to get the moving average n_steps : int number of steps across which to get the moving average Returns ------- moving_average : np.array the moving average across values \"\"\" moving_average = np . zeros ( values . shape ) for i in range ( values . shape [ 0 ]): start = np . max (( i - np . ceil ( n_steps / 2 ), 0 )) . astype ( int ) stop = np . min (( i + np . ceil ( n_steps / 2 ), values . shape [ 0 ])) . astype ( int ) moving_average [ i ] = np . mean ( values [ start : stop ]) return moving_average initialize_progress ( max_iter ) Track various stats of the progress of training the model. Parameters max_iter : int the maximum number of training iterations Returns stats : dict a dictionary of progress statistics Source code in scLEMBAS/utilities.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def initialize_progress ( max_iter : int ): \"\"\"Track various stats of the progress of training the model. Parameters ---------- max_iter : int the maximum number of training iterations Returns ------- stats : dict a dictionary of progress statistics \"\"\" stats = {} stats [ 'start_time' ] = time . time () stats [ 'end_time' ] = 0 stats [ 'iter_time' ] = np . nan * np . ones ( max_iter ) stats [ 'loss_mean' ] = np . nan * np . ones ( max_iter ) stats [ 'loss_sigma' ] = np . nan * np . ones ( max_iter ) stats [ 'eig_mean' ] = np . nan * np . ones ( max_iter ) stats [ 'eig_sigma' ] = np . nan * np . ones ( max_iter ) stats [ 'test' ] = np . nan * np . ones ( max_iter ) stats [ 'learning_rate' ] = np . nan * np . ones ( max_iter ) stats [ 'violations' ] = np . nan * np . ones ( max_iter ) return stats print_stats ( stats , iter ) Prints various stats of the progress of training the model. Parameters stats : dict a dictionary of progress statistics iter : int the current training iteration Source code in scLEMBAS/utilities.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def print_stats ( stats , iter ): \"\"\"Prints various stats of the progress of training the model. Parameters ---------- stats : dict a dictionary of progress statistics iter : int the current training iteration \"\"\" msg = 'i= {:.0f} ' . format ( iter ) if not np . isnan ( stats [ 'loss_mean' ][ iter ]): msg += ', l= {:.5f} ' . format ( stats [ 'loss_mean' ][ iter ]) # if not np.isnan(stats['test'][iter]): # msg += ', t={:.5f}'.format(stats['test'][iter]) if not np . isnan ( stats [ 'eig_mean' ][ iter ]): msg += ', s= {:.3f} ' . format ( stats [ 'eig_mean' ][ iter ]) if not np . isnan ( stats [ 'learning_rate' ][ iter ]): msg += ', r= {:.5f} ' . format ( stats [ 'learning_rate' ][ iter ]) if not np . isnan ( stats [ 'violations' ][ iter ]): msg += ', v= {:.0f} ' . format ( stats [ 'violations' ][ iter ]) print ( msg ) set_cores ( n_cores ) Set environmental variables to ensure core usage is limited to n_cores Parameters n_cores : int number of cores to use Source code in scLEMBAS/utilities.py 24 25 26 27 28 29 30 31 32 33 34 35 36 def set_cores ( n_cores : int ): \"\"\"Set environmental variables to ensure core usage is limited to n_cores Parameters ---------- n_cores : int number of cores to use \"\"\" os . environ [ \"OMP_NUM_THREADS\" ] = str ( n_cores ) os . environ [ \"MKL_NUM_THREADS\" ] = str ( n_cores ) os . environ [ \"OPENBLAS_NUM_THREADS\" ] = str ( n_cores ) os . environ [ \"VECLIB_MAXIMUM_THREADS\" ] = str ( n_cores ) os . environ [ \"NUMEXPR_NUM_THREADS\" ] = str ( n_cores ) set_seeds ( seed = 888 ) Sets random seeds for torch operations. Parameters seed : int, optional seed value, by default 888 Source code in scLEMBAS/utilities.py 11 12 13 14 15 16 17 18 19 20 21 22 def set_seeds ( seed : int = 888 ): \"\"\"Sets random seeds for torch operations. Parameters ---------- seed : int, optional seed value, by default 888 \"\"\" if 'CUBLAS_WORKSPACE_CONFIG' not in os . environ . keys (): os . environ [ 'CUBLAS_WORKSPACE_CONFIG' ] = ':4096:8' torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) update_progress ( stats , iter , loss = None , eig = None , learning_rate = None , n_sign_mismatches = None ) Updates various stats of the progress of training the model. Parameters stats : dict a dictionary of progress statistics iter : int the current training iteration loss : List[float], optional a list of the loss (excluding regularizations) up to iter , by default None eig : List[float], optional a list of the spectral_radius up to iter , by default None learning_rate : float, optional the model learning rate at iter , by default None n_sign_mismatches : float, optional the total number of sign mismatches at iter , output of SignalingModel.signaling_network.count_sign_mismatch() , by default None Returns stats : dict updated dictionary of progress statistics Source code in scLEMBAS/utilities.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def update_progress ( stats : dict , iter : int , loss : List [ float ] = None , eig : List [ float ] = None , learning_rate : float = None , n_sign_mismatches : float = None ): \"\"\"Updates various stats of the progress of training the model. Parameters ---------- stats : dict a dictionary of progress statistics iter : int the current training iteration loss : List[float], optional a list of the loss (excluding regularizations) up to `iter` , by default None eig : List[float], optional a list of the spectral_radius up to `iter` , by default None learning_rate : float, optional the model learning rate at `iter`, by default None n_sign_mismatches : float, optional the total number of sign mismatches at `iter`, output of `SignalingModel.signaling_network.count_sign_mismatch()`, by default None Returns ------- stats : dict updated dictionary of progress statistics \"\"\" if loss != None : stats [ 'loss_mean' ][ iter ] = np . mean ( np . array ( loss )) stats [ 'loss_sigma' ][ iter ] = np . std ( np . array ( loss )) if eig != None : stats [ 'eig_mean' ][ iter ] = np . mean ( np . array ( eig )) stats [ 'eig_sigma' ][ iter ] = np . std ( np . array ( eig )) if learning_rate != None : stats [ 'learning_rate' ][ iter ] = learning_rate if n_sign_mismatches != None : stats [ 'violations' ][ iter ] = n_sign_mismatches stats [ 'iter_time' ][ iter ] = time . time () return stats","title":"API"},{"location":"api/#scLEMBAS.io","text":"Read and write python objects.","title":"io"},{"location":"api/#scLEMBAS.io.read_pickled_object","text":"Read an object as a pickled file.","title":"read_pickled_object"},{"location":"api/#scLEMBAS.io.read_pickled_object--parameters","text":"file_name : str 'full/path/to/file.pickle'","title":"Parameters"},{"location":"api/#scLEMBAS.io.read_pickled_object--returns","text":"pickled_object the pickled object Source code in scLEMBAS/io.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def read_pickled_object ( file_name : str ): \"\"\"Read an object as a pickled file. Parameters ---------- file_name : str 'full/path/to/file.pickle' Returns ------- pickled_object the pickled object \"\"\" with open ( file_name , 'rb' ) as handle : pickled_object = pickle . load ( handle ) return pickled_object","title":"Returns"},{"location":"api/#scLEMBAS.io.write_pickled_object","text":"Save an object as a pickled file.","title":"write_pickled_object"},{"location":"api/#scLEMBAS.io.write_pickled_object--parameters","text":"object : Any object to save file_name : str 'full/path/to/file.pickle' Source code in scLEMBAS/io.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def write_pickled_object ( object : Any , file_name : str ) -> None : \"\"\"Save an object as a pickled file. Parameters ---------- object : Any object to save file_name : str 'full/path/to/file.pickle' \"\"\" if '.' in file_name : p = pathlib . Path ( file_name ) extensions = \"\" . join ( p . suffixes ) file_name = str ( p ) . replace ( extensions , '.pickle' ) else : file_name = file_name + '.pickle' with open ( file_name , 'wb' ) as handle : pickle . dump ( object , handle )","title":"Parameters"},{"location":"api/#scLEMBAS.model","text":"","title":"model"},{"location":"api/#scLEMBAS.model.activation_functions","text":"Defines various activations functions to be used in neural net.","title":"activation_functions"},{"location":"api/#scLEMBAS.model.activation_functions.MML_activation","text":"Returns the output of the Michaelis-Menten function","title":"MML_activation"},{"location":"api/#scLEMBAS.model.activation_functions.MML_activation--parameters","text":"x : torch.Tensor a vector of input values leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU","title":"Parameters"},{"location":"api/#scLEMBAS.model.activation_functions.MML_activation--returns","text":"fx : torch.Tensor a vector of output values Source code in scLEMBAS/model/activation_functions.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def MML_activation ( x : torch . Tensor , leak : Union [ float , int ]): \"\"\"Returns the output of the Michaelis-Menten function Parameters ---------- x : torch.Tensor a vector of input values leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU Returns ------- fx : torch.Tensor a vector of output values \"\"\" fx = torch . nn . functional . leaky_relu ( input = x , negative_slope = leak , inplace = False ) shifted_x = 0.5 * ( fx - 0.5 ) mask = torch . lt ( shifted_x , 0.0 ) gated_x = fx + 10 * mask #prevents division by 0 issue on next line right_values = 0.5 + torch . div ( shifted_x , gated_x ) fx = mask * ( fx - right_values ) + right_values #-fx trick from relu return fx","title":"Returns"},{"location":"api/#scLEMBAS.model.activation_functions.MML_delta_activation","text":"Returns the derivative of the Michaelis-Menten function","title":"MML_delta_activation"},{"location":"api/#scLEMBAS.model.activation_functions.MML_delta_activation--parameters","text":"x : torch.Tensor a vector of input values leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU","title":"Parameters"},{"location":"api/#scLEMBAS.model.activation_functions.MML_delta_activation--returns","text":"y : torch.Tensor a vector of output derivative values Source code in scLEMBAS/model/activation_functions.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def MML_delta_activation ( x : torch . Tensor , leak : Union [ float , int ]): \"\"\"Returns the derivative of the Michaelis-Menten function Parameters ---------- x : torch.Tensor a vector of input values leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU Returns ------- y : torch.Tensor a vector of output derivative values \"\"\" mask1 = x . le ( 0 ) y = torch . ones ( x . shape , dtype = x . dtype , device = x . device ) #derivative = 1 if nothing else is stated mask2 = x . gt ( 0.5 ) right_values = 0.25 / torch . pow ( x + 1e-12 , 2 ) - 1 # add psuedocount bc x = 0 will creat NaN y = y + mask2 * right_values y = y - ( 1 - leak ) * mask1 return y","title":"Returns"},{"location":"api/#scLEMBAS.model.activation_functions.MML_onestepdelta_activation_factor","text":"Adjusts weights for linearization in the spectral radius. Note that this will only work for monotonic functions","title":"MML_onestepdelta_activation_factor"},{"location":"api/#scLEMBAS.model.activation_functions.MML_onestepdelta_activation_factor--parameters","text":"Y_full : torch.Tensor description leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01","title":"Parameters"},{"location":"api/#scLEMBAS.model.activation_functions.MML_onestepdelta_activation_factor--returns","text":"y : torch.Tensor description Source code in scLEMBAS/model/activation_functions.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def MML_onestepdelta_activation_factor ( Y_full : torch . Tensor , leak : Union [ float , int ] = 0.01 ): \"\"\"Adjusts weights for linearization in the spectral radius. Note that this will only work for monotonic functions Parameters ---------- Y_full : torch.Tensor _description_ leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01 Returns ------- y : torch.Tensor _description_ \"\"\" y = torch . ones_like ( Y_full ) piece1 = Y_full . le ( 0 ) piece3 = Y_full . gt ( 0.5 ) safe_x = torch . clamp ( 1 - Y_full , max = 0.9999 ) right_values = 4 * torch . pow ( safe_x , 2 ) - 1 y = y + piece3 * right_values y = y - ( 1 - leak ) * piece1 return y","title":"Returns"},{"location":"api/#scLEMBAS.model.activation_functions.leakyReLU_activation","text":"Returns the output of the leaky ReLU function","title":"leakyReLU_activation"},{"location":"api/#scLEMBAS.model.activation_functions.leakyReLU_activation--parameters","text":"x : Iterable[Union[float, int]] a vector of input values leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01","title":"Parameters"},{"location":"api/#scLEMBAS.model.activation_functions.leakyReLU_activation--returns","text":"fx a vector of output values Source code in scLEMBAS/model/activation_functions.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def leakyReLU_activation ( x , leak = 0.01 ): \"\"\"Returns the output of the leaky ReLU function Parameters ---------- x : Iterable[Union[float, int]] a vector of input values leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01 Returns ------- fx a vector of output values \"\"\" fx = np . copy ( x ) fx = np . where ( fx < 0 , fx * leak , fx ) return fx","title":"Returns"},{"location":"api/#scLEMBAS.model.activation_functions.leakyReLU_delta_activation","text":"Returns the derivative of the leaky ReLU function","title":"leakyReLU_delta_activation"},{"location":"api/#scLEMBAS.model.activation_functions.leakyReLU_delta_activation--parameters","text":"x : Iterable[Union[float, int]] a vector of input values leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01","title":"Parameters"},{"location":"api/#scLEMBAS.model.activation_functions.leakyReLU_delta_activation--returns","text":"y : Iterable[Union[float, int]] a vector of output derivative values Source code in scLEMBAS/model/activation_functions.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def leakyReLU_delta_activation ( x , leak = 0.01 ): \"\"\"Returns the derivative of the leaky ReLU function Parameters ---------- x : Iterable[Union[float, int]] a vector of input values leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01 Returns ------- y : Iterable[Union[float, int]] a vector of output derivative values \"\"\" y = np . ones ( x . shape ) #derivative = 1 if nothing else is stated y = np . where ( x <= 0 , leak , y ) #let derivative be 0.01 at x=0 return y","title":"Returns"},{"location":"api/#scLEMBAS.model.activation_functions.sigmoid_activation","text":"Returns the output of the sigmoid function","title":"sigmoid_activation"},{"location":"api/#scLEMBAS.model.activation_functions.sigmoid_activation--parameters","text":"x : Iterable[Union[float, int]] a vector of input values leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01","title":"Parameters"},{"location":"api/#scLEMBAS.model.activation_functions.sigmoid_activation--returns","text":"fx a vector of output values Source code in scLEMBAS/model/activation_functions.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def sigmoid_activation ( x , leak = 0 ): \"\"\"Returns the output of the sigmoid function Parameters ---------- x : Iterable[Union[float, int]] a vector of input values leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01 Returns ------- fx a vector of output values \"\"\" fx = np . copy ( x ) #leak is not used for sigmoid_ fx = 1 / ( 1 + np . exp ( - fx )) return fx","title":"Returns"},{"location":"api/#scLEMBAS.model.activation_functions.sigmoid_delta_activation","text":"Returns the derivative of the sigmoid function","title":"sigmoid_delta_activation"},{"location":"api/#scLEMBAS.model.activation_functions.sigmoid_delta_activation--parameters","text":"x : Iterable[Union[float, int]] a vector of input values leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01","title":"Parameters"},{"location":"api/#scLEMBAS.model.activation_functions.sigmoid_delta_activation--returns","text":"y : Iterable[Union[float, int]] a vector of output derivative values Source code in scLEMBAS/model/activation_functions.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def sigmoid_delta_activation ( x , leak = 0 ): \"\"\"Returns the derivative of the sigmoid function Parameters ---------- x : Iterable[Union[float, int]] a vector of input values leak : Union[float, int], optional parameter to tune extent of leaking, analogous to leaky ReLU, by default 0.01 Returns ------- y : Iterable[Union[float, int]] a vector of output derivative values \"\"\" y = sigmoid_activation ( x ) * ( 1 - sigmoid_activation ( x )) return y","title":"Returns"},{"location":"api/#scLEMBAS.model.model_utilities","text":"Helper functions for building the model.","title":"model_utilities"},{"location":"api/#scLEMBAS.model.model_utilities.format_network","text":"Formats the standard network file format to that needed by SignalingModel.parse_network","title":"format_network"},{"location":"api/#scLEMBAS.model.model_utilities.format_network--parameters","text":"net : pd.DataFrame signaling network adjacency list with the following columns: - weight_label : whether the interaction is stimulating (1) or inhibiting (-1) or unknown (0.1). Exclude non-interacting (0) nodes. - stimulation_label : binary whether an interaction is stimulating (1) or [not stimultaing or unknown] (0) - inhibition_label : binary whether an interaction is inhibiting (1) or [not inhibiting or unknown] (0) weight_label : str, optional converts stimulation_label and inhibition_label to a single column of stimulating (1), inhibiting (-1), or unknown (0.1), by default 'mode_of_action' stimulation_label : str, optional column name of stimulating interactions, see net , by default 'stimulation' inhibition_label : str, optional column name of inhibitory interactions, see net , by default 'inhibition'","title":"Parameters"},{"location":"api/#scLEMBAS.model.model_utilities.format_network--returns","text":"formatted_net : pd.DataFrame the same dataframe with the additional weight_label column Source code in scLEMBAS/model/model_utilities.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def format_network ( net : pd . DataFrame , weight_label : str = 'mode_of_action' , stimulation_label : str = 'stimulation' , inhibition_label : str = 'inhibition' ) -> pd . DataFrame : \"\"\"Formats the standard network file format to that needed by `SignalingModel.parse_network` Parameters ---------- net : pd.DataFrame signaling network adjacency list with the following columns: - `weight_label`: whether the interaction is stimulating (1) or inhibiting (-1) or unknown (0.1). Exclude non-interacting (0) nodes. - `stimulation_label`: binary whether an interaction is stimulating (1) or [not stimultaing or unknown] (0) - `inhibition_label`: binary whether an interaction is inhibiting (1) or [not inhibiting or unknown] (0) weight_label : str, optional converts `stimulation_label` and `inhibition_label` to a single column of stimulating (1), inhibiting (-1), or unknown (0.1), by default 'mode_of_action' stimulation_label : str, optional column name of stimulating interactions, see `net`, by default 'stimulation' inhibition_label : str, optional column name of inhibitory interactions, see `net`, by default 'inhibition' Returns ------- formatted_net : pd.DataFrame the same dataframe with the additional `weight_label` column \"\"\" if net [( net [ stimulation_label ] == 1 ) & ( net [ inhibition_label ] == 1 )] . shape [ 0 ] > 0 : raise ValueError ( 'An interaction can either be stimulating (1,0), inhibition (0,1) or unknown (0,0)' ) formatted_net = net . copy () formatted_net [ weight_label ] = np . zeros ( net . shape [ 0 ]) formatted_net . loc [ formatted_net [ stimulation_label ] == 1 , weight_label ] = 1 formatted_net . loc [ formatted_net [ inhibition_label ] == 1 , weight_label ] = - 1 #ensuring that lack of known MOA does not imply lack of representation in scipy.sparse.find(A) formatted_net [ weight_label ] = formatted_net [ weight_label ] . replace ( 0 , 0.1 ) formatted_net [ weight_label ] = formatted_net [ weight_label ] . replace ( np . nan , 0.1 ) return formatted_net","title":"Returns"},{"location":"api/#scLEMBAS.model.model_utilities.np_to_torch","text":"Convert a numpy array to a torch.tensor","title":"np_to_torch"},{"location":"api/#scLEMBAS.model.model_utilities.np_to_torch--parameters","text":"arr : np.array torch.dtype, optional datatype to store values in torch, by default torch.float32 device : str whether to use gpu (\"cuda\") or cpu (\"cpu\"), by default \"cpu\" Source code in scLEMBAS/model/model_utilities.py 10 11 12 13 14 15 16 17 18 19 20 21 22 def np_to_torch ( arr : np . array , dtype : torch . float32 , device : str = 'cpu' ): \"\"\"Convert a numpy array to a torch.tensor Parameters ---------- arr : np.array dtype : torch.dtype, optional datatype to store values in torch, by default torch.float32 device : str whether to use gpu (\"cuda\") or cpu (\"cpu\"), by default \"cpu\" \"\"\" return torch . tensor ( arr , dtype = dtype , device = device )","title":"Parameters"},{"location":"api/#scLEMBAS.model.train","text":"Train the signaling model.","title":"train"},{"location":"api/#scLEMBAS.model.train.ModelData","text":"Bases: Dataset Source code in scLEMBAS/model/train.py 65 66 67 68 69 70 71 72 73 74 class ModelData ( Dataset ): def __init__ ( self , X_in , y_out ): self . X_in = X_in self . y_out = y_out def __len__ ( self ) -> int : \"Returns the total number of samples.\" return self . X_in . shape [ 0 ] def __getitem__ ( self , idx : int ): \"Returns one sample of data, data and label (X, y).\" return self . X_in [ idx , :], self . y_out [ idx , :]","title":"ModelData"},{"location":"api/#scLEMBAS.model.train.ModelData.__getitem__","text":"Returns one sample of data, data and label (X, y). Source code in scLEMBAS/model/train.py 72 73 74 def __getitem__ ( self , idx : int ): \"Returns one sample of data, data and label (X, y).\" return self . X_in [ idx , :], self . y_out [ idx , :]","title":"__getitem__"},{"location":"api/#scLEMBAS.model.train.ModelData.__len__","text":"Returns the total number of samples. Source code in scLEMBAS/model/train.py 69 70 71 def __len__ ( self ) -> int : \"Returns the total number of samples.\" return self . X_in . shape [ 0 ]","title":"__len__"},{"location":"api/#scLEMBAS.model.train.split_data","text":"Splits the data into train, test, and validation.","title":"split_data"},{"location":"api/#scLEMBAS.model.train.split_data--parameters","text":"X_in : torch.Tensor input ligand concentrations. Index represents samples and columns represent a ligand. Values represent amount of ligand introduced (e.g., concentration). y_out : torch.Tensor output TF activities. Index represents samples and columns represent TFs. Values represent activity of the TF. train_split_frac : Dict, optional fraction of samples to be assigned to each of train, test and split, by default 0.8, 0.2, and 0 respectively seed : int, optional seed value, by default 888 Source code in scLEMBAS/model/train.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def split_data ( X_in : torch . Tensor , y_out : torch . Tensor , train_split_frac : Dict = { 'train' : 0.8 , 'test' : 0.2 , 'validation' : None }, seed : int = 888 ): \"\"\"Splits the data into train, test, and validation. Parameters ---------- X_in : torch.Tensor input ligand concentrations. Index represents samples and columns represent a ligand. Values represent amount of ligand introduced (e.g., concentration). y_out : torch.Tensor output TF activities. Index represents samples and columns represent TFs. Values represent activity of the TF. train_split_frac : Dict, optional fraction of samples to be assigned to each of train, test and split, by default 0.8, 0.2, and 0 respectively seed : int, optional seed value, by default 888 \"\"\" if not np . isclose ( sum ([ v for v in train_split_frac . values () if v ]), 1 ): raise ValueError ( 'Train-test-validation split must sum to 1' ) if not train_split_frac [ 'validation' ] or train_split_frac [ 'validation' ] == 0 : X_train , X_test , y_train , y_test = train_test_split ( X_in , y_out , train_size = train_split_frac [ 'train' ], random_state = seed ) X_val , y_val = None , None else : X_train , _X , y_train , _y = train_test_split ( X_in , y_out , train_size = train_split_frac [ 'train' ], random_state = seed ) X_test , X_val , y_test , y_val = train_test_split ( _X , _y , train_size = train_split_frac [ 'test' ] / ( train_split_frac [ 'test' ] + train_split_frac [ 'validation' ]), random_state = seed ) return X_train , X_test , X_val , y_train , y_test , y_val","title":"Parameters"},{"location":"api/#scLEMBAS.model.train.train_signaling_model","text":"Trains the signaling model","title":"train_signaling_model"},{"location":"api/#scLEMBAS.model.train.train_signaling_model--parameters","text":"mod : SignalingModel initialized signaling model. Suggested to also run mod.signaling_network.prescale_weights prior to training optimizer : torch.optim.adam.Adam optimizer to use during training loss_fn : torch.nn.modules.loss.MSELoss loss function to use during training reset_epoch : int, optional number of epochs upon which to reset the optimizer state, by default 200 hyper_params : Dict[str, Union[int, float]], optional various hyper parameter inputs for training - 'max_iter' : the number of epochs, by default 5000 - 'learning_rate' : the starting learning rate, by default 2e-3 - 'batch_size' : number of samples per batch, by default 8 - 'noise_level' : noise added to signaling network input, by default 10. Set to 0 for no noise. Makes model more robust. - 'gradient_noise_level' : noise added to gradient after backward pass. Makes model more robust. - 'reset_epoch' : number of epochs upon which to reset the optimizer state, by default 200 - 'param_lambda_L2' : L2 regularization penalty term for most of the model weights and biases - 'moa_lambda_L1' : L1 regularization penalty term for incorrect interaction mechanism of action (inhibiting/stimulating) - 'ligand_lambda_L2' : L2 regularization penalty term for ligand biases - 'uniform_lambda_L2' : L2 regularization penalty term for - 'uniform_max' : - 'spectral_loss_factor' : regularization penalty term for - 'n_probes_spectral' : - 'power_steps_spectral' : - 'subset_n_spectral' : train_split_frac : Dict, optional fraction of samples to be assigned to each of train, test and split, by default 0.8, 0.2, and 0 respectively train_seed : int, optional seed value, by default mod.seed. By explicitly making this an argument, it allows different train-test splits even with the same mod.seed, e.g., for cross-validation verbose : bool, optional whether to print various progress stats across training epochs","title":"Parameters"},{"location":"api/#scLEMBAS.model.train.train_signaling_model--returns","text":"mod : SignalingModel a copy of the input model with trained parameters cur_loss : List[float], optional a list of the loss (excluding regularizations) across training iterations cur_eig : List[float], optional a list of the spectral_radius across training iterations mean_loss : torch.Tensor mean TF activity loss across samples (independent of training) X_train : torch.Tensor the train split of the input data X_test : torch.Tensor the test split of the input data X_val : torch.Tensor the validation split of the input data y_train : torch.Tensor the train split of the output data y_test : torch.Tensor the test split of the output data y_val : torch.Tensor the validation split of the output data Source code in scLEMBAS/model/train.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def train_signaling_model ( mod , optimizer : torch . optim , loss_fn : torch . nn . modules . loss , reset_epoch : int = 200 , hyper_params : Dict [ str , Union [ int , float ]] = None , train_split_frac : Dict = { 'train' : 0.8 , 'test' : 0.2 , 'validation' : None }, train_seed : int = None , verbose : bool = True ): \"\"\"Trains the signaling model Parameters ---------- mod : SignalingModel initialized signaling model. Suggested to also run `mod.signaling_network.prescale_weights` prior to training optimizer : torch.optim.adam.Adam optimizer to use during training loss_fn : torch.nn.modules.loss.MSELoss loss function to use during training reset_epoch : int, optional number of epochs upon which to reset the optimizer state, by default 200 hyper_params : Dict[str, Union[int, float]], optional various hyper parameter inputs for training - 'max_iter' : the number of epochs, by default 5000 - 'learning_rate' : the starting learning rate, by default 2e-3 - 'batch_size' : number of samples per batch, by default 8 - 'noise_level' : noise added to signaling network input, by default 10. Set to 0 for no noise. Makes model more robust. - 'gradient_noise_level' : noise added to gradient after backward pass. Makes model more robust. - 'reset_epoch' : number of epochs upon which to reset the optimizer state, by default 200 - 'param_lambda_L2' : L2 regularization penalty term for most of the model weights and biases - 'moa_lambda_L1' : L1 regularization penalty term for incorrect interaction mechanism of action (inhibiting/stimulating) - 'ligand_lambda_L2' : L2 regularization penalty term for ligand biases - 'uniform_lambda_L2' : L2 regularization penalty term for - 'uniform_max' : - 'spectral_loss_factor' : regularization penalty term for - 'n_probes_spectral' : - 'power_steps_spectral' : - 'subset_n_spectral' : train_split_frac : Dict, optional fraction of samples to be assigned to each of train, test and split, by default 0.8, 0.2, and 0 respectively train_seed : int, optional seed value, by default mod.seed. By explicitly making this an argument, it allows different train-test splits even with the same mod.seed, e.g., for cross-validation verbose : bool, optional whether to print various progress stats across training epochs Returns ------- mod : SignalingModel a copy of the input model with trained parameters cur_loss : List[float], optional a list of the loss (excluding regularizations) across training iterations cur_eig : List[float], optional a list of the spectral_radius across training iterations mean_loss : torch.Tensor mean TF activity loss across samples (independent of training) X_train : torch.Tensor the train split of the input data X_test : torch.Tensor the test split of the input data X_val : torch.Tensor the validation split of the input data y_train : torch.Tensor the train split of the output data y_test : torch.Tensor the test split of the output data y_val : torch.Tensor the validation split of the output data \"\"\" if not hyper_params : hyper_params = HYPER_PARAMS . copy () else : hyper_params = { k : v for k , v in { ** HYPER_PARAMS , ** hyper_params } . items () if k in HYPER_PARAMS } # give user input priority stats = utils . initialize_progress ( hyper_params [ 'max_iter' ]) mod = mod . copy () # do not overwrite input optimizer = optimizer ( mod . parameters (), lr = 1 , weight_decay = 0 ) reset_state = optimizer . state . copy () X_in = mod . df_to_tensor ( mod . X_in ) y_out = mod . df_to_tensor ( mod . y_out ) mean_loss = loss_fn ( torch . mean ( y_out , dim = 0 ) * torch . ones ( y_out . shape , device = y_out . device ), y_out ) # mean TF (across samples) loss # set up data objects if not train_seed : train_seed = mod . seed X_train , X_test , X_val , y_train , y_test , y_val = split_data ( X_in , y_out , train_split_frac , train_seed ) train_data = ModelData ( X_train . to ( 'cpu' ), y_train . to ( 'cpu' )) if mod . device == 'cuda' : pin_memory = True else : pin_memory = False # if n_cores != 0: # n_cores_train = min(n_cores, hyper_params['batch_size']) # else: # n_cores_train = n_cores train_dataloader = DataLoader ( dataset = train_data , batch_size = hyper_params [ 'batch_size' ], # num_workers=n_cores_train, drop_last = False , pin_memory = pin_memory , shuffle = True ) start_time = time . time () # begin iteration for e in range ( hyper_params [ 'max_iter' ]): # set learning rate cur_lr = utils . get_lr ( e , hyper_params [ 'max_iter' ], max_height = hyper_params [ 'learning_rate' ], start_height = hyper_params [ 'learning_rate' ] / 10 , end_height = 1e-6 , peak = 1000 ) optimizer . param_groups [ 0 ][ 'lr' ] = cur_lr cur_loss = [] cur_eig = [] # iterate through batches if mod . seed : utils . set_seeds ( mod . seed + e ) for batch , ( X_in_ , y_out_ ) in enumerate ( train_dataloader ): mod . train () optimizer . zero_grad () X_in_ , y_out_ = X_in_ . to ( mod . device ), y_out_ . to ( mod . device ) # forward pass X_full = mod . input_layer ( X_in_ ) # transform to full network with ligand input concentrations utils . set_seeds ( mod . seed + mod . _gradient_seed_counter ) network_noise = torch . randn ( X_full . shape , device = X_full . device ) X_full = X_full + ( hyper_params [ 'noise_level' ] * cur_lr * network_noise ) # randomly add noise to signaling network input, makes model more robust Y_full = mod . signaling_network ( X_full ) # train signaling network weights Y_hat = mod . output_layer ( Y_full ) # get prediction loss fit_loss = loss_fn ( y_out_ , Y_hat ) # get regularization losses sign_reg = mod . signaling_network . sign_regularization ( lambda_L1 = hyper_params [ 'moa_lambda_L1' ]) # incorrect MoA ligand_reg = mod . ligand_regularization ( lambda_L2 = hyper_params [ 'ligand_lambda_L2' ]) # ligand biases stability_loss , spectral_radius = mod . signaling_network . get_SS_loss ( Y_full = Y_full . detach (), spectral_loss_factor = hyper_params [ 'spectral_loss_factor' ], subset_n = hyper_params [ 'subset_n_spectral' ], n_probes = hyper_params [ 'n_probes_spectral' ], power_steps = hyper_params [ 'power_steps_spectral' ]) uniform_reg = mod . uniform_regularization ( lambda_L2 = hyper_params [ 'uniform_lambda_L2' ] * cur_lr , Y_full = Y_full , target_min = 0 , target_max = hyper_params [ 'uniform_max' ]) # uniform distribution param_reg = mod . L2_reg ( hyper_params [ 'param_lambda_L2' ]) # all model weights and signaling network biases total_loss = fit_loss + sign_reg + ligand_reg + param_reg + stability_loss + uniform_reg # gradient total_loss . backward () mod . add_gradient_noise ( noise_level = hyper_params [ 'gradient_noise_level' ]) optimizer . step () # store cur_eig . append ( spectral_radius ) cur_loss . append ( fit_loss . item ()) stats = utils . update_progress ( stats , iter = e , loss = cur_loss , eig = cur_eig , learning_rate = cur_lr , n_sign_mismatches = mod . signaling_network . count_sign_mismatch ()) if verbose and e % 250 == 0 : utils . print_stats ( stats , iter = e ) if np . logical_and ( e % reset_epoch == 0 , e > 0 ): optimizer . state = reset_state . copy () if verbose : mins , secs = divmod ( time . time () - start_time , 60 ) print ( \"Training ran in: {:.0f} min {:.2f} sec\" . format ( mins , secs )) return mod , cur_loss , cur_eig , mean_loss , stats , X_train , X_test , X_val , y_train , y_test , y_val","title":"Returns"},{"location":"api/#scLEMBAS.plotting","text":"Helper functions for data visualization.","title":"plotting"},{"location":"api/#scLEMBAS.plotting.shade_plot","text":"summary","title":"shade_plot"},{"location":"api/#scLEMBAS.plotting.shade_plot--parameters","text":"X : np.array x axis values Y : np.array y axis values sigma : np.array standard deviation of y axis values x_label : str x axis label y_label : str y axis label","title":"Parameters"},{"location":"api/#scLEMBAS.plotting.shade_plot--returns","text":"plot : plotnine.ggplot.ggplot description Source code in scLEMBAS/plotting.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def shade_plot ( X : np . array , Y : np . array , sigma : np . array , x_label : str , y_label : str , width : int = 5 , height : int = 3 ): \"\"\"_summary_ Parameters ---------- X : np.array x axis values Y : np.array y axis values sigma : np.array standard deviation of y axis values x_label : str x axis label y_label : str y axis label Returns ------- plot : plotnine.ggplot.ggplot _description_ \"\"\" data = pd . DataFrame ( data = { x_label : X , y_label : Y , 'sigma' : sigma }) data [ 'sigma_min' ] = data [ y_label ] - data . sigma data [ 'sigma_max' ] = data [ y_label ] + data . sigma plot = ( p9 . ggplot ( data , p9 . aes ( x = x_label )) + p9 . geom_line ( p9 . aes ( y = y_label ), color = '#1E90FF' ) + p9 . geom_ribbon ( p9 . aes ( y = y_label , ymin = 'sigma_min' , ymax = 'sigma_max' ), alpha = 0.2 ) + p9 . xlim ([ 0 , data . shape [ 0 ]]) + # p9.ylim(10**min_log, round(data[y_label].max(), 1)) + # p9.scale_y_log10() + p9 . theme_bw () + p9 . theme ( figure_size = ( width , height )) ) return plot","title":"Returns"},{"location":"api/#scLEMBAS.preprocess","text":"Preprocessing functions for single-cell AnnData objects.","title":"preprocess"},{"location":"api/#scLEMBAS.preprocess.get_tf_activity","text":"Wrapper of decoupler to estimate TF activity from single-cell transcriptomics data.","title":"get_tf_activity"},{"location":"api/#scLEMBAS.preprocess.get_tf_activity--parameters","text":"adata : AnnData Annotated single-cell data matrix organism : str The organism of interest: either NCBI Taxonomy ID, common name, latin name or Ensembl name. Organisms other than human will be translated from human data by orthology. grn : str, optional database to get the GRN, by default 'collectri'. Available options are collectri or dorothea . min_n : int Minimum of targets per source. If less, sources are removed. By default 5. verbose : bool Whether to show progress. use_raw : bool Use AnnData its raw attribute. filter_pvals : bool whether to set TF activity estimates to 0 if it is insignificant according to pval_thresh pval_thresh : float significance threshold, by default 0.05. Used in conjunction with filter_pvals. kwargs : passed to decoupler.decouple .","title":"Parameters"},{"location":"api/#scLEMBAS.preprocess.get_tf_activity--returns","text":"estimate : DataFrame Consensus TF activity scores. Stored in .obsm['consensus_estimate'] . pvals : DataFrame Obtained TF activity p-values. Stored in .obsm['consensus_pvals'] . Source code in scLEMBAS/preprocess.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def get_tf_activity ( adata , organism : str , grn = 'collectri' , verbose : bool = True , min_n : int = 5 , use_raw : bool = False , filter_pvals : bool = False , pval_thresh : float = 0.05 , ** kwargs ): \"\"\"Wrapper of decoupler to estimate TF activity from single-cell transcriptomics data. Parameters ---------- adata : AnnData Annotated single-cell data matrix organism : str The organism of interest: either NCBI Taxonomy ID, common name, latin name or Ensembl name. Organisms other than human will be translated from human data by orthology. grn : str, optional database to get the GRN, by default 'collectri'. Available options are ``collectri`` or ``dorothea``. min_n : int Minimum of targets per source. If less, sources are removed. By default 5. verbose : bool Whether to show progress. use_raw : bool Use AnnData its raw attribute. filter_pvals : bool whether to set TF activity estimates to 0 if it is insignificant according to pval_thresh pval_thresh : float significance threshold, by default 0.05. Used in conjunction with filter_pvals. kwargs : passed to `decoupler.decouple`. Returns ------- estimate : DataFrame Consensus TF activity scores. Stored in `.obsm['consensus_estimate']`. pvals : DataFrame Obtained TF activity p-values. Stored in `.obsm['consensus_pvals']`. \"\"\" grn_map = { 'collectri' : dc . get_collectri , 'dorothea' : dc . get_dorothea } # get_dorothea returns \"A\" confidence by default net = grn_map [ grn ]( organism = organism , split_complexes = False ) # builds on dorothea, used by Saez-Rodriguez lab # # reimplementation of dc.run_consensus, allowing all options in dc.decouple to be passed # dc.run_consensus(mat=adata, net=net, source='source', target='target', weight='weight', **kwargs) # m, r, c = extract(adata, use_raw=use_raw, verbose=verbose) if verbose : print ( 'Running consensus.' ) # # unnecessary, this is the default behavior # if not kwargs: # kwargs = {'methods': ['lm', 'ulm', 'wsum'], # 'cns_metds': ['lm', 'ulm', 'wsum_norm']} # else: # if 'methods' not in kwargs: # kwargs['methods'] = ['lm', 'ulm', 'wsum'] # if 'cns_methods' not in kwargs and kwargs['methods'] == ['lm', 'ulm', 'wsum']: # kwargs['cns_metds'] = ['lm', 'ulm', 'wsum_norm'] dc . decouple ( mat = adata , net = net , source = 'source' , target = 'target' , weight = 'weight' , consensus = True , min_n = min_n , verbose = verbose , use_raw = use_raw , ** kwargs ) if filter_pvals : estimate_key = [ k for k in adata . obsm if k . endswith ( '_estimate' )] pvals_key = [ k for k in adata . obsm if k . endswith ( '_pvals' )] if len ( estimate_key ) > 0 or len ( pvals_key ) > 0 : warnings . warn ( 'Multiple TF estimates/pvals, choosing first' ) adata . obsm [ estimate_key [ 0 ]][ adata . obsm [ pvals_key [ 0 ]] > pval_thresh ] = 0","title":"Returns"},{"location":"api/#scLEMBAS.preprocess.project_tf_activity","text":"Runs dimensionality reduction and clustering of cells from their TF activity using default scanpy parameters.","title":"project_tf_activity"},{"location":"api/#scLEMBAS.preprocess.project_tf_activity--parameters","text":"adata : AnnData AnnData object with TF activity scores stored in .obsm[estimate_key] . estimate_key : str, optional .obsm key under which TF activity is stored, by default 'consensus_estimate'","title":"Parameters"},{"location":"api/#scLEMBAS.preprocess.project_tf_activity--returns","text":"tf_adata : AnnData AnnData object with input TF activity estimates stored in .X , and dimensionality reduction and clustering outputs stored in default scanpy locations. Cluster labels on TF activity space are stores in adata.obs['TF_clusters'] Source code in scLEMBAS/preprocess.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def project_tf_activity ( adata : AnnData , estimate_key : str = 'consensus_estimate' ): \"\"\"Runs dimensionality reduction and clustering of cells from their TF activity using default scanpy parameters. Parameters ---------- adata : AnnData AnnData object with TF activity scores stored in `.obsm[estimate_key]`. estimate_key : str, optional `.obsm` key under which TF activity is stored, by default 'consensus_estimate' Returns ------- tf_adata : AnnData AnnData object with input TF activity estimates stored in `.X`, and dimensionality reduction and clustering outputs stored in default scanpy locations. Cluster labels on TF activity space are stores in `adata.obs['TF_clusters']` \"\"\" tf_adata = AnnData ( adata . obsm [ estimate_key ]) sc . tl . pca ( data = tf_adata ) # _pca_simple(adata = tf_adata) # run PCA # only needed for the model to use transform, but sc.tl.ingest can do this pc_rank = _compute_elbow ( adata = tf_adata ) tf_adata . uns [ \"pca\" ][ 'pca_rank' ] = pc_rank # if not np.allclose(tf_adata.obsm['X_pca'], tf_adata.uns['pca']['pca_mod'].transform(tf_adata.X)): # raise ValueError('Unexpected disagreement when running PCA.transform') sc . pp . neighbors ( adata = tf_adata , n_pcs = pc_rank ) # construct neighborhood graph sc . tl . umap ( adata = tf_adata ) # run UMAP sc . tl . leiden ( adata = tf_adata ) # cluster tf_adata . obs . rename ( columns = { 'leiden' : 'TF_clusters' }, inplace = True ) tf_adata . obs = pd . concat ([ adata . obs , tf_adata . obs ], axis = 1 ) return tf_adata","title":"Returns"},{"location":"api/#scLEMBAS.utilities","text":"Helper functions for running and training the SignalingModel.","title":"utilities"},{"location":"api/#scLEMBAS.utilities.get_lr","text":"Calculates learning rate for a given iteration during training.","title":"get_lr"},{"location":"api/#scLEMBAS.utilities.get_lr--parameters","text":"iter : int the current iteration max_iter : int the maximum number of training iterations max_height : float, optional tuning parameters for learning for the first 95% of iterations, by default 1e-3 start_height : float, optional tuning parameter for learning rate before peak iterations, by default 1e-5 end_height : float, optional tuning parameter for learning rate afer peak iterations, by default 1e-5 peak : int, optional the first # of iterations to calculate lr on (should be less than 95% of max_iter), by default 1000","title":"Parameters"},{"location":"api/#scLEMBAS.utilities.get_lr--returns","text":"lr : float the learning rate Source code in scLEMBAS/utilities.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def get_lr ( iter : int , max_iter : int , max_height : float = 1e-3 , start_height : float = 1e-5 , end_height : float = 1e-5 , peak : int = 1000 ): \"\"\"Calculates learning rate for a given iteration during training. Parameters ---------- iter : int the current iteration max_iter : int the maximum number of training iterations max_height : float, optional tuning parameters for learning for the first 95% of iterations, by default 1e-3 start_height : float, optional tuning parameter for learning rate before peak iterations, by default 1e-5 end_height : float, optional tuning parameter for learning rate afer peak iterations, by default 1e-5 peak : int, optional the first # of iterations to calculate lr on (should be less than 95% of max_iter), by default 1000 Returns ------- lr : float the learning rate \"\"\" phase_length = 0.95 * max_iter if iter <= peak : effective_iter = iter / peak lr = ( max_height - start_height ) * 0.5 * ( np . cos ( np . pi * ( effective_iter + 1 )) + 1 ) + start_height elif iter <= phase_length : effective_iter = ( iter - peak ) / ( phase_length - peak ) lr = ( max_height - end_height ) * 0.5 * ( np . cos ( np . pi * ( effective_iter + 2 )) + 1 ) + end_height else : lr = end_height return lr","title":"Returns"},{"location":"api/#scLEMBAS.utilities.get_moving_average","text":"Get the moving average of a tracked state across n_steps. Serves to smooth value.","title":"get_moving_average"},{"location":"api/#scLEMBAS.utilities.get_moving_average--parameters","text":"values : np.array values on which to get the moving average n_steps : int number of steps across which to get the moving average","title":"Parameters"},{"location":"api/#scLEMBAS.utilities.get_moving_average--returns","text":"moving_average : np.array the moving average across values Source code in scLEMBAS/utilities.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def get_moving_average ( values : np . array , n_steps : int ): \"\"\"Get the moving average of a tracked state across n_steps. Serves to smooth value. Parameters ---------- values : np.array values on which to get the moving average n_steps : int number of steps across which to get the moving average Returns ------- moving_average : np.array the moving average across values \"\"\" moving_average = np . zeros ( values . shape ) for i in range ( values . shape [ 0 ]): start = np . max (( i - np . ceil ( n_steps / 2 ), 0 )) . astype ( int ) stop = np . min (( i + np . ceil ( n_steps / 2 ), values . shape [ 0 ])) . astype ( int ) moving_average [ i ] = np . mean ( values [ start : stop ]) return moving_average","title":"Returns"},{"location":"api/#scLEMBAS.utilities.initialize_progress","text":"Track various stats of the progress of training the model.","title":"initialize_progress"},{"location":"api/#scLEMBAS.utilities.initialize_progress--parameters","text":"max_iter : int the maximum number of training iterations","title":"Parameters"},{"location":"api/#scLEMBAS.utilities.initialize_progress--returns","text":"stats : dict a dictionary of progress statistics Source code in scLEMBAS/utilities.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def initialize_progress ( max_iter : int ): \"\"\"Track various stats of the progress of training the model. Parameters ---------- max_iter : int the maximum number of training iterations Returns ------- stats : dict a dictionary of progress statistics \"\"\" stats = {} stats [ 'start_time' ] = time . time () stats [ 'end_time' ] = 0 stats [ 'iter_time' ] = np . nan * np . ones ( max_iter ) stats [ 'loss_mean' ] = np . nan * np . ones ( max_iter ) stats [ 'loss_sigma' ] = np . nan * np . ones ( max_iter ) stats [ 'eig_mean' ] = np . nan * np . ones ( max_iter ) stats [ 'eig_sigma' ] = np . nan * np . ones ( max_iter ) stats [ 'test' ] = np . nan * np . ones ( max_iter ) stats [ 'learning_rate' ] = np . nan * np . ones ( max_iter ) stats [ 'violations' ] = np . nan * np . ones ( max_iter ) return stats","title":"Returns"},{"location":"api/#scLEMBAS.utilities.print_stats","text":"Prints various stats of the progress of training the model.","title":"print_stats"},{"location":"api/#scLEMBAS.utilities.print_stats--parameters","text":"stats : dict a dictionary of progress statistics iter : int the current training iteration Source code in scLEMBAS/utilities.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def print_stats ( stats , iter ): \"\"\"Prints various stats of the progress of training the model. Parameters ---------- stats : dict a dictionary of progress statistics iter : int the current training iteration \"\"\" msg = 'i= {:.0f} ' . format ( iter ) if not np . isnan ( stats [ 'loss_mean' ][ iter ]): msg += ', l= {:.5f} ' . format ( stats [ 'loss_mean' ][ iter ]) # if not np.isnan(stats['test'][iter]): # msg += ', t={:.5f}'.format(stats['test'][iter]) if not np . isnan ( stats [ 'eig_mean' ][ iter ]): msg += ', s= {:.3f} ' . format ( stats [ 'eig_mean' ][ iter ]) if not np . isnan ( stats [ 'learning_rate' ][ iter ]): msg += ', r= {:.5f} ' . format ( stats [ 'learning_rate' ][ iter ]) if not np . isnan ( stats [ 'violations' ][ iter ]): msg += ', v= {:.0f} ' . format ( stats [ 'violations' ][ iter ]) print ( msg )","title":"Parameters"},{"location":"api/#scLEMBAS.utilities.set_cores","text":"Set environmental variables to ensure core usage is limited to n_cores","title":"set_cores"},{"location":"api/#scLEMBAS.utilities.set_cores--parameters","text":"n_cores : int number of cores to use Source code in scLEMBAS/utilities.py 24 25 26 27 28 29 30 31 32 33 34 35 36 def set_cores ( n_cores : int ): \"\"\"Set environmental variables to ensure core usage is limited to n_cores Parameters ---------- n_cores : int number of cores to use \"\"\" os . environ [ \"OMP_NUM_THREADS\" ] = str ( n_cores ) os . environ [ \"MKL_NUM_THREADS\" ] = str ( n_cores ) os . environ [ \"OPENBLAS_NUM_THREADS\" ] = str ( n_cores ) os . environ [ \"VECLIB_MAXIMUM_THREADS\" ] = str ( n_cores ) os . environ [ \"NUMEXPR_NUM_THREADS\" ] = str ( n_cores )","title":"Parameters"},{"location":"api/#scLEMBAS.utilities.set_seeds","text":"Sets random seeds for torch operations.","title":"set_seeds"},{"location":"api/#scLEMBAS.utilities.set_seeds--parameters","text":"seed : int, optional seed value, by default 888 Source code in scLEMBAS/utilities.py 11 12 13 14 15 16 17 18 19 20 21 22 def set_seeds ( seed : int = 888 ): \"\"\"Sets random seeds for torch operations. Parameters ---------- seed : int, optional seed value, by default 888 \"\"\" if 'CUBLAS_WORKSPACE_CONFIG' not in os . environ . keys (): os . environ [ 'CUBLAS_WORKSPACE_CONFIG' ] = ':4096:8' torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed )","title":"Parameters"},{"location":"api/#scLEMBAS.utilities.update_progress","text":"Updates various stats of the progress of training the model.","title":"update_progress"},{"location":"api/#scLEMBAS.utilities.update_progress--parameters","text":"stats : dict a dictionary of progress statistics iter : int the current training iteration loss : List[float], optional a list of the loss (excluding regularizations) up to iter , by default None eig : List[float], optional a list of the spectral_radius up to iter , by default None learning_rate : float, optional the model learning rate at iter , by default None n_sign_mismatches : float, optional the total number of sign mismatches at iter , output of SignalingModel.signaling_network.count_sign_mismatch() , by default None","title":"Parameters"},{"location":"api/#scLEMBAS.utilities.update_progress--returns","text":"stats : dict updated dictionary of progress statistics Source code in scLEMBAS/utilities.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def update_progress ( stats : dict , iter : int , loss : List [ float ] = None , eig : List [ float ] = None , learning_rate : float = None , n_sign_mismatches : float = None ): \"\"\"Updates various stats of the progress of training the model. Parameters ---------- stats : dict a dictionary of progress statistics iter : int the current training iteration loss : List[float], optional a list of the loss (excluding regularizations) up to `iter` , by default None eig : List[float], optional a list of the spectral_radius up to `iter` , by default None learning_rate : float, optional the model learning rate at `iter`, by default None n_sign_mismatches : float, optional the total number of sign mismatches at `iter`, output of `SignalingModel.signaling_network.count_sign_mismatch()`, by default None Returns ------- stats : dict updated dictionary of progress statistics \"\"\" if loss != None : stats [ 'loss_mean' ][ iter ] = np . mean ( np . array ( loss )) stats [ 'loss_sigma' ][ iter ] = np . std ( np . array ( loss )) if eig != None : stats [ 'eig_mean' ][ iter ] = np . mean ( np . array ( eig )) stats [ 'eig_sigma' ][ iter ] = np . std ( np . array ( eig )) if learning_rate != None : stats [ 'learning_rate' ][ iter ] = learning_rate if n_sign_mismatches != None : stats [ 'violations' ][ iter ] = n_sign_mismatches stats [ 'iter_time' ][ iter ] = time . time () return stats","title":"Returns"}]}